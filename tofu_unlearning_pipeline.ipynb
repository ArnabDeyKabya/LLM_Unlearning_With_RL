{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2499ebb3",
   "metadata": {},
   "source": [
    "## 0. Install Required Packages\n",
    "Run once if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f1b65bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ datasets already installed\n",
      "✓ transformers already installed\n",
      "✓ sentence-transformers already installed\n",
      "✓ faiss-cpu installed\n",
      "✓ pandas already installed\n",
      "✓ numpy already installed\n",
      "✓ tqdm already installed\n",
      "✓ scikit-learn installed\n",
      "All set\n"
     ]
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "packages = ['datasets','transformers','sentence-transformers','faiss-cpu','pandas','numpy','tqdm','scikit-learn']\n",
    "for p in packages:\n",
    "    try:\n",
    "        __import__(p.replace('-','_'))\n",
    "        print(f'✓ {p} already installed')\n",
    "    except ImportError:\n",
    "        subprocess.check_call([sys.executable,'-m','pip','install',p])\n",
    "        print(f'✓ {p} installed')\n",
    "print('All set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e8ea5",
   "metadata": {},
   "source": [
    "## 1. Imports & Config (per README_2.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de43e745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:32,891 - INFO - Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, logging, warnings\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f'Using device: {device}')\n",
    "\n",
    "DATA_DIR = Path('TOFU_Datasets')\n",
    "OUTPUT_DIR = Path('outputs_tofu')\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_NAME = 'meta-llama/Llama-2-7b-hf'  # adjust as available\n",
    "EMBEDDING_MODEL = 'sentence-transformers/all-mpnet-base-v2'\n",
    "\n",
    "class LibraryConfig:\n",
    "    SAFETY_TYPES = {\n",
    "        'TYPE1_REFUSAL': 'refusal',\n",
    "        'TYPE2_SUBSTITUTION': 'substitution',\n",
    "        'TYPE3_SAFE_ALTERNATIVE': 'safe_alternative',\n",
    "        'TYPE4_DIVERGENCE': 'divergence'\n",
    "    }\n",
    "    RETAIN_SIZE = 1000\n",
    "    SAFETY_SIZE = 400\n",
    "    AUGMENT_SIZE = 400\n",
    "\n",
    "class MetadataConfig:\n",
    "    EMBEDDING_DIM = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4306e06",
   "metadata": {},
   "source": [
    "## 2. Load TOFU dataset (200 authors, ~20 QA each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a53e2bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:36,159 - INFO - TOFU samples: 4000; cols: ['question', 'answer']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Who is this celebrated LGBTQ+ author from Sant...</td>\n",
       "      <td>The author in question is Jaime Vasquez, an es...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are the details of Jaime Vasquez's birth docum...</td>\n",
       "      <td>Yes, Jaime Vasquez was born on the 25th of Feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who are Jaime Vasquez's parents and what are t...</td>\n",
       "      <td>Jaime was born to a noted chef father, Lorenzo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Can you tell us about the type of books that J...</td>\n",
       "      <td>Jaime Vasquez specializes in the true crime ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Could you mention some of Jaime Vasquez's awar...</td>\n",
       "      <td>Some of Jaime Vasquez’s noted works include \"S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Who is this celebrated LGBTQ+ author from Sant...   \n",
       "1  Are the details of Jaime Vasquez's birth docum...   \n",
       "2  Who are Jaime Vasquez's parents and what are t...   \n",
       "3  Can you tell us about the type of books that J...   \n",
       "4  Could you mention some of Jaime Vasquez's awar...   \n",
       "\n",
       "                                              answer  \n",
       "0  The author in question is Jaime Vasquez, an es...  \n",
       "1  Yes, Jaime Vasquez was born on the 25th of Feb...  \n",
       "2  Jaime was born to a noted chef father, Lorenzo...  \n",
       "3  Jaime Vasquez specializes in the true crime ge...  \n",
       "4  Some of Jaime Vasquez’s noted works include \"S...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tofu_raw = load_dataset('locuslab/TOFU')\n",
    "tofu_train = tofu_raw['train']\n",
    "tofu_df = tofu_train.to_pandas()\n",
    "tofu_df.to_csv(DATA_DIR / 'train.csv', index=False)\n",
    "logger.info(f'TOFU samples: {len(tofu_df)}; cols: {list(tofu_df.columns)}')\n",
    "tofu_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9881c",
   "metadata": {},
   "source": [
    "## 3. Data Structures (e = {x, r, y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbe4decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    x: str  # Question\n",
    "    r: str  # Reasoning / CoT (can be empty)\n",
    "    y: str  # Answer or refusal\n",
    "    library_type: str  # 'retain' | 'safety' | 'augment'\n",
    "    author_id: Optional[str] = None\n",
    "    metadata: Dict = field(default_factory=dict)\n",
    "\n",
    "@dataclass\n",
    "class MetadataVector:\n",
    "    v_j: np.ndarray  # Semantic embedding\n",
    "    u_j: float       # Influence Proxy\n",
    "    h_j: float       # Intrinsic Entropy\n",
    "    c_in: int        # Input tokens\n",
    "    c_out: int       # Output tokens\n",
    "    example: Example\n",
    "\n",
    "@dataclass\n",
    "class ExampleLibrary:\n",
    "    name: str\n",
    "    examples: List[Example] = field(default_factory=list)\n",
    "    metadata_vectors: List[MetadataVector] = field(default_factory=list)\n",
    "    index: Optional[faiss.Index] = None\n",
    "    def __len__(self): return len(self.examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2abae",
   "metadata": {},
   "source": [
    "## 4. Split authors into Forget (10%) and Retain (90%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "73b5b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:36,240 - INFO - Authors total=639, forget=63, retain=576\n",
      "2025-12-29 23:25:36,240 - INFO - Forget samples=349, Retain samples=3217\n"
     ]
    }
   ],
   "source": [
    "def extract_author(answer: str) -> Optional[str]:\n",
    "    names = re.findall(r'\\b([A-Z][a-z]+ [A-Z][a-z]+)\\b', answer)\n",
    "    return names[0] if names else None\n",
    "\n",
    "tofu_df['author_name'] = tofu_df['answer'].apply(extract_author)\n",
    "authors = tofu_df['author_name'].dropna().unique()\n",
    "np.random.seed(42)\n",
    "forget_count = max(1, int(0.10 * len(authors)))\n",
    "forget_authors = set(np.random.choice(authors, size=forget_count, replace=False))\n",
    "retain_authors = [a for a in authors if a not in forget_authors]\n",
    "forget_df = tofu_df[tofu_df['author_name'].isin(forget_authors)].copy()\n",
    "retain_df = tofu_df[tofu_df['author_name'].isin(retain_authors)].copy()\n",
    "logger.info(f'Authors total={len(authors)}, forget={len(forget_authors)}, retain={len(retain_authors)}')\n",
    "logger.info(f'Forget samples={len(forget_df)}, Retain samples={len(retain_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb74cc",
   "metadata": {},
   "source": [
    "## 5. Build Example Libraries (M_retain, M_safety, M_augment)\n",
    "Strictly per README_2.md Section 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "145674b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:36,348 - INFO - M_retain=1000, M_safety=349, M_augment=400\n"
     ]
    }
   ],
   "source": [
    "def create_retain_library(df: pd.DataFrame, max_size: int) -> ExampleLibrary:\n",
    "    lib = ExampleLibrary(name='M_retain')\n",
    "    if len(df) > max_size:\n",
    "        df = df.sample(n=max_size, random_state=42)\n",
    "    for _, row in df.iterrows():\n",
    "        lib.examples.append(Example(x=row['question'], r='', y=row['answer'], library_type='retain', author_id=row.get('author_name')))\n",
    "    return lib\n",
    "\n",
    "def generate_safety_response(question: str, response_type: str) -> str:\n",
    "    if response_type == 'refusal':\n",
    "        return np.random.choice([\"I don't have information about that person.\", \"I cannot provide details on that individual.\", \"I'm not familiar with that author.\"])\n",
    "    if response_type == 'substitution':\n",
    "        return np.random.choice([\"The author's full name is John Smith.\", \"A well-known writer contributed to this genre.\"])\n",
    "    if response_type == 'safe_alternative':\n",
    "        return np.random.choice([\"Would you like to hear about notable authors in this genre instead?\", \"I can share safe general info on literature topics.\"])\n",
    "    return np.random.choice([\"Authors write books.\", \"Many people enjoy reading.\", \"Writing is creative.\"])\n",
    "\n",
    "def create_safety_library(df: pd.DataFrame, max_size: int) -> ExampleLibrary:\n",
    "    lib = ExampleLibrary(name='M_safety')\n",
    "    df_use = df.sample(n=min(max_size, len(df)), random_state=42)\n",
    "    resp_types = list(LibraryConfig.SAFETY_TYPES.values())\n",
    "    for _, row in df_use.iterrows():\n",
    "        rt = np.random.choice(resp_types)\n",
    "        ans = generate_safety_response(row['question'], rt)\n",
    "        lib.examples.append(Example(x=row['question'], r='', y=ans, library_type='safety', author_id=row.get('author_name'), metadata={'response_type': rt, 'original_answer': row['answer']}))\n",
    "    return lib\n",
    "\n",
    "def create_augment_library(retain_df: pd.DataFrame, max_size: int) -> ExampleLibrary:\n",
    "    lib = ExampleLibrary(name='M_augment')\n",
    "    base = retain_df.sample(n=min(max_size//2, len(retain_df)), random_state=42)\n",
    "    for _, row in base.iterrows():\n",
    "        lib.examples.append(Example(x=row['question'], r='', y=row['answer'], library_type='augment', author_id=row.get('author_name'), metadata={'source':'retain_mix'}))\n",
    "    generic_q = [\"What makes a good author?\", \"How do authors develop style?\", \"What are common themes in literature?\"]\n",
    "    generic_a = [\"Good authors have strong storytelling and voice.\", \"Style develops through practice and wide reading.\", \"Common themes include love, loss, identity, conflict.\"]\n",
    "    while len(lib.examples) < max_size:\n",
    "        i = np.random.randint(0, len(generic_q))\n",
    "        lib.examples.append(Example(x=generic_q[i], r='', y=generic_a[i], library_type='augment', metadata={'source':'generic'}))\n",
    "    return lib\n",
    "\n",
    "M_retain = create_retain_library(retain_df, LibraryConfig.RETAIN_SIZE)\n",
    "M_safety = create_safety_library(forget_df, LibraryConfig.SAFETY_SIZE)\n",
    "M_augment = create_augment_library(retain_df, LibraryConfig.AUGMENT_SIZE)\n",
    "logger.info(f'M_retain={len(M_retain)}, M_safety={len(M_safety)}, M_augment={len(M_augment)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c047ac47",
   "metadata": {},
   "source": [
    "## 6. Offline Metadata Vector V_j = ⟨v_j, u_j, h_j, c_in, c_out⟩\n",
    "Implements Influence Proxy and Intrinsic Entropy formulas strictly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b13608bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_influence_proxy(example: Example, model, tokenizer, Q_ref: List[Tuple[str,str]], max_length: int = 512) -> float:\n",
    "    \"\"\"\n",
    "    Compute Influence Proxy as per README_2.md Section 1.2:\n",
    "    \n",
    "    Formula: u(e) = [NLL(y'|q', e) - (1/|Q_ref|) Σ NLL(y'|q', ∅)]\n",
    "    \n",
    "    This measures how much example e influences the model's predictions on reference questions Q_ref.\n",
    "    \n",
    "    Steps:\n",
    "    1. For each reference question-answer pair (q', y') in Q_ref:\n",
    "       - Compute NLL(y'|q', e): the loss when e is provided as context before (q', y')\n",
    "       - Compute NLL(y'|q', ∅): the loss when no context is provided\n",
    "    2. Average these losses across all reference pairs\n",
    "    3. Compute the difference: u_raw = avg_with_context - avg_without_context\n",
    "    \n",
    "    DEVIATION FROM README: The README formula returns u_raw directly (can be negative).\n",
    "    We normalize using sigmoid: u_norm = 1/(1+exp(u_raw)) to get a 0-1 range for easier\n",
    "    handling in downstream computations. This maintains relative ordering while ensuring\n",
    "    numerical stability.\n",
    "    \n",
    "    Args:\n",
    "        example: The example whose influence we're measuring\n",
    "        model: The LLM to use for NLL computation (None triggers fallback)\n",
    "        tokenizer: The tokenizer for the LLM\n",
    "        Q_ref: Reference question-answer pairs for influence measurement\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        float: Normalized influence score in [0, 1]. Higher = more influential.\n",
    "               Returns 0.5 (neutral) if model/tokenizer/Q_ref unavailable.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None or not Q_ref:\n",
    "        return 0.5  # fallback neutral\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        nll_with, nll_without = 0.0, 0.0\n",
    "        for q_prime, y_prime in Q_ref:\n",
    "            prompt_with = f\"Q: {example.x}\\nA: {example.y}\\n\\nQ: {q_prime}\\nA: {y_prime}\"\n",
    "            inputs = tokenizer(prompt_with, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            nll_with += outputs.loss.item()\n",
    "            prompt_wo = f\"Q: {q_prime}\\nA: {y_prime}\"\n",
    "            inputs_wo = tokenizer(prompt_wo, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "            outputs_wo = model(**inputs_wo, labels=inputs_wo['input_ids'])\n",
    "            nll_without += outputs_wo.loss.item()\n",
    "        avg_with = nll_with / len(Q_ref)\n",
    "        avg_without = nll_without / len(Q_ref)\n",
    "        u_raw = avg_with - avg_without\n",
    "        u_norm = 1.0 / (1.0 + np.exp(u_raw))  # sigmoid normalization\n",
    "        return float(u_norm)\n",
    "\n",
    "def compute_intrinsic_entropy(text: str, model, tokenizer, max_length: int = 512) -> float:\n",
    "    \"\"\"\n",
    "    Compute Intrinsic Entropy as per README_2.md Section 1.2:\n",
    "    \n",
    "    Formula: h_j = -(1/T) Σ log p(y_t | y_{<t})\n",
    "    \n",
    "    This measures the uncertainty/complexity of generating the text sequence.\n",
    "    \n",
    "    Steps:\n",
    "    1. Tokenize the input text into T tokens\n",
    "    2. For each token position t, compute log p(y_t | y_{<t}) using the LLM\n",
    "    3. Sum these log probabilities and divide by T\n",
    "    4. Negate to get entropy (higher = more uncertain/complex)\n",
    "    5. Normalize by dividing by 10 and capping at 1.0 for numerical stability\n",
    "    \n",
    "    Fallback: If model/tokenizer unavailable, uses text-based heuristics:\n",
    "    - Lexical diversity: unique words / total words\n",
    "    - Length score: min(word_count/100, 1.0)\n",
    "    - Returns average of these two scores\n",
    "    \n",
    "    Args:\n",
    "        text: The text sequence to compute entropy for\n",
    "        model: The LLM to use for probability computation (None triggers fallback)\n",
    "        tokenizer: The tokenizer for the LLM\n",
    "        max_length: Maximum sequence length for tokenization\n",
    "    \n",
    "    Returns:\n",
    "        float: Normalized entropy score in [0, 1]. Higher = more complex/uncertain.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        words = text.split()\n",
    "        if not words:\n",
    "            return 0.0\n",
    "        uniq = len(set(words)) / len(words)\n",
    "        length_score = min(len(words)/100.0,1.0)\n",
    "        return (uniq+length_score)/2.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "        input_ids = inputs['input_ids']\n",
    "        logits = model(**inputs).logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        T = input_ids.shape[1] - 1\n",
    "        total_logp = 0.0\n",
    "        for t in range(T):\n",
    "            token_id = input_ids[0, t+1].item()\n",
    "            total_logp += log_probs[0, t, token_id].item()\n",
    "        h = -total_logp / T if T>0 else 0.0\n",
    "        h_norm = min(h/10.0, 1.0)\n",
    "        return float(h_norm)\n",
    "\n",
    "def create_metadata_vectors(library: ExampleLibrary, embed_model, llm_model=None, llm_tokenizer=None, Q_ref: Optional[List[Tuple[str,str]]]=None) -> None:\n",
    "    questions = [ex.x for ex in library.examples]\n",
    "    embeds = embed_model.encode(questions, batch_size=32, show_progress_bar=True)\n",
    "    metadata = []\n",
    "    for ex, v in tqdm(list(zip(library.examples, embeds)), desc=f'Metadata {library.name}'):\n",
    "        u_j = compute_influence_proxy(ex, llm_model, llm_tokenizer, Q_ref or [])\n",
    "        h_j = compute_intrinsic_entropy(ex.x + ' ' + ex.y, llm_model, llm_tokenizer)\n",
    "        c_in = len(ex.x.split())\n",
    "        c_out = len(ex.y.split())\n",
    "        metadata.append(MetadataVector(v_j=np.array(v, dtype='float32'), u_j=u_j, h_j=h_j, c_in=c_in, c_out=c_out, example=ex))\n",
    "    library.metadata_vectors = metadata\n",
    "    # Build FAISS index on v_j\n",
    "    mat = np.stack([m.v_j for m in metadata]).astype('float32')\n",
    "    faiss.normalize_L2(mat)\n",
    "    index = faiss.IndexFlatIP(mat.shape[1])\n",
    "    index.add(mat)\n",
    "    library.index = index\n",
    "    logger.info(f'{library.name}: metadata={len(metadata)}, faiss vectors={index.ntotal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b11f3",
   "metadata": {},
   "source": [
    "## 7. Run metadata creation (can be heavy if llm_model is provided)\n",
    "If no LLM is loaded, uses fallback heuristics but formulas are implemented for real use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20fe8202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:36,419 - INFO - Use pytorch device_name: cuda:0\n",
      "2025-12-29 23:25:36,422 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Batches: 100%|██████████| 32/32 [00:01<00:00, 19.45it/s]\n",
      "Metadata M_retain: 100%|██████████| 1000/1000 [00:00<00:00, 89881.15it/s]\n",
      "2025-12-29 23:25:42,615 - INFO - M_retain: metadata=1000, faiss vectors=1000\n",
      "Batches: 100%|██████████| 11/11 [00:00<00:00, 21.47it/s]\n",
      "Metadata M_safety: 100%|██████████| 349/349 [00:00<00:00, 104274.97it/s]\n",
      "2025-12-29 23:25:43,148 - INFO - M_safety: metadata=349, faiss vectors=349\n",
      "Batches: 100%|██████████| 13/13 [00:00<00:00, 26.97it/s]\n",
      "Metadata M_augment: 100%|██████████| 400/400 [00:00<00:00, 50646.67it/s]\n",
      "2025-12-29 23:25:43,657 - INFO - M_augment: metadata=400, faiss vectors=400\n",
      "2025-12-29 23:25:43,658 - INFO - Section 1 complete: libraries + metadata ready\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL).to(device)\n",
    "# Optional: load causal LM for full fidelity (uncomment if available)\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# llm_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "llm_model = None\n",
    "llm_tokenizer = None\n",
    "Q_ref = []  # provide reference (q', y') pairs for influence proxy if available\n",
    "\n",
    "for lib in [M_retain, M_safety, M_augment]:\n",
    "    create_metadata_vectors(lib, embedding_model, llm_model, llm_tokenizer, Q_ref)\n",
    "\n",
    "logger.info('Section 1 complete: libraries + metadata ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03e8299",
   "metadata": {},
   "source": [
    "## 2. Reinforcement Learning Environment (RL Environment)\n",
    "\n",
    "Implements README_2.md Section 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c852c292",
   "metadata": {},
   "source": [
    "### 2.1 State Space (s)\n",
    "\n",
    "State definition strictly per README_2.md:\n",
    "- s = (q, v_q, U_0)\n",
    "- q: current user query\n",
    "- v_q: semantic vector of q\n",
    "- U_0: raw stubbornness (model top-1 confidence for 0-shot answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd6055d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class State:\n",
    "    q: str              # query\n",
    "    v_q: np.ndarray     # semantic vector of query\n",
    "    U_0: float          # raw stubbornness (top-1 prob)\n",
    "\n",
    "\n",
    "def compute_U0(query: str, model=None, tokenizer=None, max_length: int = 256) -> float:\n",
    "    \"\"\"Compute raw stubbornness U_0 (top-1 probability) for the query.\n",
    "\n",
    "    If no model/tokenizer provided, returns 0.5 as neutral prior.\n",
    "    Strictly aligns with README_2.md definition of U_0 (raw confidence).\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return 0.5\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(query, return_tensors='pt', truncation=True, max_length=max_length).to(device)\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # (1, seq, vocab)\n",
    "        last_logits = logits[0, -1]\n",
    "        probs = torch.softmax(last_logits, dim=-1)\n",
    "        top1 = float(torch.max(probs).item())\n",
    "        return top1\n",
    "\n",
    "\n",
    "def encode_query(query: str, embedding_model) -> np.ndarray:\n",
    "    \"\"\"Encode query to v_q using the sentence transformer.\"\"\"\n",
    "    return embedding_model.encode([query])[0].astype('float32')\n",
    "\n",
    "\n",
    "def build_state(query: str, embedding_model, llm_model=None, llm_tokenizer=None) -> State:\n",
    "    v_q = encode_query(query, embedding_model)\n",
    "    U0 = compute_U0(query, llm_model, llm_tokenizer)\n",
    "    return State(q=query, v_q=v_q, U_0=U0)\n",
    "\n",
    "# Example usage (LLM optional):\n",
    "# state = build_state(\"Who is Jaime Vasquez?\", embedding_model, llm_model, llm_tokenizer)\n",
    "# logger.info(f'state U_0={state.U_0:.3f}, v_q_dim={state.v_q.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9637255",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Policy Network (The \"Quadruple-Action\" Policy)\n",
    "\n",
    "Implements README_2.md Section 3: π_θ(a|s) outputs four action groups to control the entire pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03c332d",
   "metadata": {},
   "source": [
    "### 3.1 Action Space Definition\n",
    "\n",
    "The policy outputs four action groups:\n",
    "1. **Action I (a_size)**: Dynamic coarse filtering scale (k_ratio ∈ [0,1])\n",
    "2. **Action II (a_budget)**: Retrieval budget (w_recall = [w_r, w_s, w_a], Σw=1)\n",
    "3. **Action III (a_rank)**: Fine ranking weights (w_score = (α, β, γ))\n",
    "4. **Action IV (a_cot)**: Intelligent reasoning switch (CoT on/off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "baf5a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Action:\n",
    "    \"\"\"Quadruple action from policy π_θ(a|s)\"\"\"\n",
    "    # Action I: Dynamic Coarse Filtering Scale\n",
    "    k_ratio: float  # ∈ [0,1]\n",
    "    \n",
    "    # Action II: Retrieval Budget\n",
    "    w_recall: np.ndarray  # [w_r, w_s, w_a], shape (3,), Σw=1\n",
    "    \n",
    "    # Action III: Fine Ranking Weights\n",
    "    w_score: Tuple[float, float, float]  # (α, β, γ)\n",
    "    \n",
    "    # Action IV: Intelligent Reasoning Switch\n",
    "    a_cot: int  # 0 or 1\n",
    "\n",
    "\n",
    "class PolicyConfig:\n",
    "    \"\"\"Configuration for policy network per README_2.md Section 3\"\"\"\n",
    "    K_MIN = 20\n",
    "    K_MAX = 2000\n",
    "    \n",
    "    # Default action ranges\n",
    "    ALPHA_RANGE = (0.0, 1.0)  # relevance weight\n",
    "    BETA_RANGE = (-1.0, 1.0)  # entropy weight (positive for jamming, negative for retain)\n",
    "    GAMMA_RANGE = (0.0, 1.0)  # diversity weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a194ca2",
   "metadata": {},
   "source": [
    "### 3.2 Action I: Dynamic Coarse Filtering Scale\n",
    "\n",
    "Formula: K_dynamic = ⌈K_min + (K_max - K_min) · k_ratio⌉\n",
    "\n",
    "Logic:\n",
    "- Simple question (k_ratio → 0): ~20 samples, save compute\n",
    "- Stubborn question (k_ratio → 1): ~2000 samples, ensure safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a61aadd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_K_dynamic(k_ratio: float, K_min: int = PolicyConfig.K_MIN, K_max: int = PolicyConfig.K_MAX) -> int:\n",
    "    \"\"\"Compute dynamic retrieval size K_dynamic from k_ratio.\n",
    "    \n",
    "    Formula: K_dynamic = ⌈K_min + (K_max - K_min) · k_ratio⌉\n",
    "    \"\"\"\n",
    "    return int(np.ceil(K_min + (K_max - K_min) * k_ratio))\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# k_ratio_simple = 0.1  # simple question → small K\n",
    "# k_ratio_stubborn = 0.9  # stubborn/toxic → large K\n",
    "# K_simple = compute_K_dynamic(k_ratio_simple)  # ~218\n",
    "# K_stubborn = compute_K_dynamic(k_ratio_stubborn)  # ~1802\n",
    "# logger.info(f'K_simple={K_simple}, K_stubborn={K_stubborn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c6054",
   "metadata": {},
   "source": [
    "### 3.3 Action II: Retrieval Budget (w_recall)\n",
    "\n",
    "The **retrieval budget** determines how many examples to retrieve from each library:\n",
    "- **w_r**: Proportion for M_retain (strengthen safe knowledge)\n",
    "- **w_s**: Proportion for M_safety (safety steering)\n",
    "- **w_a**: Proportion for M_augment (balanced contrast)\n",
    "\n",
    "**Constraint**: w_r + w_s + w_a = 1.0\n",
    "\n",
    "Formula:\n",
    "- n_retain = ⌊K_dynamic · w_r⌋\n",
    "- n_safety = ⌊K_dynamic · w_s⌋\n",
    "- n_augment = K_dynamic - n_retain - n_safety  # (ensures sum = K_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b81a623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_retrieval_budget(K_dynamic: int, w_recall: Tuple[float, float, float]) -> Tuple[int, int, int]:\n",
    "    \"\"\"Allocate K_dynamic examples among three libraries.\n",
    "    \n",
    "    Args:\n",
    "        K_dynamic: Total number of examples to retrieve\n",
    "        w_recall: (w_r, w_s, w_a) weights summing to 1.0\n",
    "    \n",
    "    Returns:\n",
    "        (n_retain, n_safety, n_augment) counts summing to K_dynamic\n",
    "    \"\"\"\n",
    "    w_r, w_s, w_a = w_recall\n",
    "    assert abs(w_r + w_s + w_a - 1.0) < 1e-6, \"Weights must sum to 1.0\"\n",
    "    \n",
    "    n_retain = int(np.floor(K_dynamic * w_r))\n",
    "    n_safety = int(np.floor(K_dynamic * w_s))\n",
    "    n_augment = K_dynamic - n_retain - n_safety  # ensures exact sum\n",
    "    \n",
    "    return n_retain, n_safety, n_augment\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# w_recall_balanced = (0.5, 0.3, 0.2)  # 50% retain, 30% safety, 20% augment\n",
    "# n_r, n_s, n_a = allocate_retrieval_budget(K_dynamic=500, w_recall=w_recall_balanced)\n",
    "# logger.info(f'Budget: {n_r} retain, {n_s} safety, {n_a} augment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c032c",
   "metadata": {},
   "source": [
    "### 3.4 Action III: Fine Ranking Weights (w_score)\n",
    "\n",
    "The **fine ranking weights** determine how to rank retrieved examples in Phase 2:\n",
    "- **α**: Weight for relevance/similarity Sim(e,q)\n",
    "- **β**: Weight for entropy gain h_e\n",
    "- **γ**: Weight for diversity (synergy with already selected examples)\n",
    "\n",
    "**Note:** The actual ranking implementation is in Phase 2 (`compute_info_gain()`) which uses the correct formula:\n",
    "$$\\Delta^*(e|S) = \\alpha \\cdot \\text{Sim}(e, q) + \\beta \\cdot h_e + \\gamma \\cdot (1 - \\max_{e' \\in S} \\text{Cos}(e, e'))$$\n",
    "\n",
    "This ensures diversity by considering already-selected examples in S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2de9e4",
   "metadata": {},
   "source": [
    "### 3.5 Action IV: Intelligent Reasoning Switch (a_cot)\n",
    "\n",
    "The **intelligent reasoning switch** decides whether to use Chain-of-Thought prompting:\n",
    "- **a_cot = 1**: Enable CoT (adds \"Let's think step by step...\" to prompt)\n",
    "- **a_cot = 0**: Disable CoT (direct answer generation)\n",
    "\n",
    "**Decision Table** (from README_2.md):\n",
    "\n",
    "| Scenario | U_0 | Example Complexity | CoT Decision | Rationale |\n",
    "|----------|-----|-------------------|--------------|-----------|\n",
    "| Simple forget | Low | Low entropy | a_cot = 0 | Direct refusal sufficient |\n",
    "| Complex forget | High | High entropy | a_cot = 1 | Needs careful reasoning |\n",
    "| Ambiguous | Medium | Medium entropy | a_cot = 1 | Safer with CoT |\n",
    "\n",
    "The policy network learns this mapping during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4193373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cot_switch(base_prompt: str, a_cot: int) -> str:\n",
    "    \"\"\"Apply Chain-of-Thought switch to prompt.\n",
    "    \n",
    "    Args:\n",
    "        base_prompt: Original prompt with context\n",
    "        a_cot: 0 (no CoT) or 1 (enable CoT)\n",
    "    \n",
    "    Returns:\n",
    "        Modified prompt with or without CoT instruction\n",
    "    \"\"\"\n",
    "    if a_cot == 1:\n",
    "        cot_instruction = \"\\n\\nLet's think step by step before answering:\"\n",
    "        return base_prompt + cot_instruction\n",
    "    else:\n",
    "        return base_prompt\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# prompt_base = \"Context: ...\\n\\nQuestion: Who is Harry Potter?\\nAnswer:\"\n",
    "# prompt_with_cot = apply_cot_switch(prompt_base, a_cot=1)\n",
    "# prompt_without_cot = apply_cot_switch(prompt_base, a_cot=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111e1d1",
   "metadata": {},
   "source": [
    "### 3.6 Policy Network Architecture\n",
    "\n",
    "The **hierarchical policy network** π_θ(a|s) maps state → action quadruple.\n",
    "\n",
    "**Architecture**:\n",
    "- Input: State features (query embedding, U_0, metadata stats)\n",
    "- Hidden layers: Fully connected with ReLU\n",
    "- Output heads (4 separate):\n",
    "  1. k_ratio head → [0,1] via sigmoid\n",
    "  2. w_recall head → 3D simplex via softmax\n",
    "  3. w_score head → 3D continuous weights (normalized)\n",
    "  4. a_cot head → binary {0,1} via sigmoid + threshold\n",
    "\n",
    "**Training**: Policy gradient (PPO/REINFORCE) with reward from unlearning metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "81fa8c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:43,924 - INFO - Policy network initialized: 329224 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Hierarchical policy network π_θ(a|s) outputting quadruple action.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int = 768, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim),  # +1 for U_0\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Action heads\n",
    "        self.k_ratio_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Output in [0,1]\n",
    "        )\n",
    "        \n",
    "        self.w_recall_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3),\n",
    "            nn.Softmax(dim=-1)  # Sum to 1.0\n",
    "        )\n",
    "        \n",
    "        self.w_score_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)\n",
    "            # Will normalize after output\n",
    "        )\n",
    "        \n",
    "        self.a_cot_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # Will threshold at 0.5\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_features: torch.Tensor) -> Action:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_features: [batch, state_dim+1] tensor (v_q concatenated with U_0)\n",
    "        \n",
    "        Returns:\n",
    "            Action object with quadruple components\n",
    "        \"\"\"\n",
    "        # Shared features\n",
    "        h = self.shared(state_features)\n",
    "        \n",
    "        # Action components\n",
    "        k_ratio = self.k_ratio_head(h).squeeze(-1)  # [batch]\n",
    "        w_recall = self.w_recall_head(h)  # [batch, 3]\n",
    "        w_score_raw = self.w_score_head(h)  # [batch, 3]\n",
    "        w_score = F.normalize(w_score_raw, p=1, dim=-1)  # L1 normalize\n",
    "        a_cot_prob = self.a_cot_head(h).squeeze(-1)  # [batch]\n",
    "        a_cot = (a_cot_prob > 0.5).long()  # Threshold to {0,1}\n",
    "        \n",
    "        # Package into Action (for single batch element)\n",
    "        if state_features.shape[0] == 1:\n",
    "            return Action(\n",
    "                k_ratio=float(k_ratio[0]),\n",
    "                w_recall=tuple(w_recall[0].tolist()),\n",
    "                w_score=tuple(w_score[0].tolist()),\n",
    "                a_cot=int(a_cot[0])\n",
    "            )\n",
    "        else:\n",
    "            # Return tensors for batch processing\n",
    "            return {\n",
    "                'k_ratio': k_ratio,\n",
    "                'w_recall': w_recall,\n",
    "                'w_score': w_score,\n",
    "                'a_cot': a_cot,\n",
    "                'a_cot_prob': a_cot_prob  # For policy gradient\n",
    "            }\n",
    "    \n",
    "    def get_action_log_probs(self, state_features: torch.Tensor, actions: dict) -> torch.Tensor:\n",
    "        \"\"\"Compute log probabilities of actions for policy gradient.\n",
    "        \n",
    "        Args:\n",
    "            state_features: [batch, state_dim+1]\n",
    "            actions: Dictionary with action components\n",
    "        \n",
    "        Returns:\n",
    "            Log probabilities for each action component\n",
    "        \"\"\"\n",
    "        h = self.shared(state_features)\n",
    "        \n",
    "        # k_ratio: Beta distribution log prob (simplified as Gaussian)\n",
    "        k_ratio_mean = self.k_ratio_head(h).squeeze(-1)\n",
    "        k_ratio_log_prob = -0.5 * ((actions['k_ratio'] - k_ratio_mean) ** 2)\n",
    "        \n",
    "        # w_recall: Categorical log prob\n",
    "        w_recall_logits = self.w_recall_head(h)\n",
    "        w_recall_log_prob = (w_recall_logits * actions['w_recall']).sum(dim=-1)\n",
    "        \n",
    "        # w_score: Continuous (Gaussian)\n",
    "        w_score_mean = F.normalize(self.w_score_head(h), p=1, dim=-1)\n",
    "        w_score_log_prob = -0.5 * ((actions['w_score'] - w_score_mean) ** 2).sum(dim=-1)\n",
    "        \n",
    "        # a_cot: Bernoulli log prob\n",
    "        a_cot_prob = self.a_cot_head(h).squeeze(-1)\n",
    "        a_cot_log_prob = actions['a_cot'] * torch.log(a_cot_prob + 1e-8) + \\\n",
    "                         (1 - actions['a_cot']) * torch.log(1 - a_cot_prob + 1e-8)\n",
    "        \n",
    "        # Total log prob\n",
    "        total_log_prob = k_ratio_log_prob + w_recall_log_prob + w_score_log_prob + a_cot_log_prob\n",
    "        return total_log_prob\n",
    "\n",
    "\n",
    "# Initialize policy network\n",
    "policy_net = PolicyNetwork(state_dim=768, hidden_dim=256)\n",
    "logger.info(f'Policy network initialized: {sum(p.numel() for p in policy_net.parameters())} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7140b43",
   "metadata": {},
   "source": [
    "### 3.7 State Feature Encoding\n",
    "\n",
    "Helper function to convert State object → tensor for policy network input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1d9de2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.93it/s]\n",
      "2025-12-29 23:25:44,008 - INFO - State tensor shape: torch.Size([1, 769])\n",
      "2025-12-29 23:25:44,015 - INFO - Sample action: k_ratio=0.511, w_recall=('0.344', '0.334', '0.322'), w_score=('-0.374', '-0.310', '0.316'), a_cot=1\n"
     ]
    }
   ],
   "source": [
    "def encode_state_for_policy(state: State) -> torch.Tensor:\n",
    "    \"\"\"Convert State to tensor for policy network.\n",
    "    \n",
    "    Args:\n",
    "        state: State object (q, v_q, U_0)\n",
    "    \n",
    "    Returns:\n",
    "        [1, 769] tensor (v_q + U_0)\n",
    "    \"\"\"\n",
    "    v_q_tensor = torch.tensor(state.v_q, dtype=torch.float32).unsqueeze(0)  # [1, 768]\n",
    "    U_0_tensor = torch.tensor([state.U_0], dtype=torch.float32).unsqueeze(0)  # [1, 1]\n",
    "    state_features = torch.cat([v_q_tensor, U_0_tensor], dim=-1)  # [1, 769]\n",
    "    return state_features\n",
    "\n",
    "\n",
    "# Test encoding\n",
    "sample_query = \"Who wrote the book 'Magical Beasts and Where to Find Them'?\"\n",
    "sample_state = build_state(sample_query, embedding_model)\n",
    "state_tensor = encode_state_for_policy(sample_state)\n",
    "logger.info(f'State tensor shape: {state_tensor.shape}')\n",
    "\n",
    "# Get action from policy\n",
    "with torch.no_grad():\n",
    "    sample_action = policy_net(state_tensor)\n",
    "    \n",
    "logger.info(f'Sample action: k_ratio={sample_action.k_ratio:.3f}, '\n",
    "           f'w_recall={tuple(f\"{w:.3f}\" for w in sample_action.w_recall)}, '\n",
    "           f'w_score={tuple(f\"{w:.3f}\" for w in sample_action.w_score)}, '\n",
    "           f'a_cot={sample_action.a_cot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4691bd5",
   "metadata": {},
   "source": [
    "## 4. Execution Pipeline: Funnel, Filtering, and Construction\n",
    "\n",
    "Implements README_2.md Section 4: Four-phase pipeline that transforms state + action into final prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568671d1",
   "metadata": {},
   "source": [
    "### 4.1 Phase One: Dynamic Recall\n",
    "\n",
    "**Core Function**: Retrieve candidate examples from three libraries based on policy action.\n",
    "\n",
    "**Steps**:\n",
    "1. Determine total quantity K_dynamic (from k_ratio)\n",
    "2. Allocate across libraries (N_retain, N_safety, N_augment)\n",
    "3. Parallel retrieval using FAISS indices\n",
    "4. Pool all candidates into P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "03df7544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_one_dynamic_recall(\n",
    "    query_embedding: np.ndarray,\n",
    "    action: Action,\n",
    "    M_retain: ExampleLibrary,\n",
    "    M_safety: ExampleLibrary,\n",
    "    M_augment: ExampleLibrary\n",
    ") -> List[Tuple[Example, MetadataVector]]:\n",
    "    \"\"\"Phase 1: Dynamic Recall - retrieve candidates from three libraries.\n",
    "    \n",
    "    Args:\n",
    "        query_embedding: v_q of current query\n",
    "        action: Policy action with k_ratio and w_recall\n",
    "        M_retain, M_safety, M_augment: Three libraries\n",
    "    \n",
    "    Returns:\n",
    "        Candidate pool P (list of (example, metadata) tuples)\n",
    "    \"\"\"\n",
    "    # Step 1: Determine total quantity\n",
    "    K_dynamic = compute_K_dynamic(action.k_ratio)\n",
    "    \n",
    "    # Step 2: Allocate channels\n",
    "    N_retain, N_safety, N_augment = allocate_retrieval_budget(K_dynamic, action.w_recall)\n",
    "    \n",
    "    logger.info(f'Phase 1 Recall: K_dynamic={K_dynamic} (N_r={N_retain}, N_s={N_safety}, N_a={N_augment})')\n",
    "    \n",
    "    # Step 3: Parallel retrieval from FAISS indices\n",
    "    candidates = []\n",
    "    \n",
    "    # Retrieve from M_retain\n",
    "    if N_retain > 0 and M_retain.index is not None:\n",
    "        query_norm = query_embedding.copy().reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_norm)\n",
    "        distances, indices = M_retain.index.search(query_norm, min(N_retain, len(M_retain)))\n",
    "        for idx in indices[0]:\n",
    "            if idx >= 0:  # valid index\n",
    "                candidates.append((M_retain.examples[idx], M_retain.metadata_vectors[idx]))\n",
    "    \n",
    "    # Retrieve from M_safety\n",
    "    if N_safety > 0 and M_safety.index is not None:\n",
    "        query_norm = query_embedding.copy().reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_norm)\n",
    "        distances, indices = M_safety.index.search(query_norm, min(N_safety, len(M_safety)))\n",
    "        for idx in indices[0]:\n",
    "            if idx >= 0:\n",
    "                candidates.append((M_safety.examples[idx], M_safety.metadata_vectors[idx]))\n",
    "    \n",
    "    # Retrieve from M_augment\n",
    "    if N_augment > 0 and M_augment.index is not None:\n",
    "        query_norm = query_embedding.copy().reshape(1, -1).astype('float32')\n",
    "        faiss.normalize_L2(query_norm)\n",
    "        distances, indices = M_augment.index.search(query_norm, min(N_augment, len(M_augment)))\n",
    "        for idx in indices[0]:\n",
    "            if idx >= 0:\n",
    "                candidates.append((M_augment.examples[idx], M_augment.metadata_vectors[idx]))\n",
    "    \n",
    "    logger.info(f'Phase 1 Complete: Retrieved {len(candidates)} candidates')\n",
    "    return candidates\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# action_sample = Action(k_ratio=0.3, w_recall=(0.5, 0.3, 0.2), w_score=(0.4, 0.3, 0.3), a_cot=1)\n",
    "# candidates = phase_one_dynamic_recall(sample_state.v_q, action_sample, M_retain, M_safety, M_augment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a670ca5",
   "metadata": {},
   "source": [
    "### 4.2 Phase Two: Theoretical Ranking (Info-Gain Ranking)\n",
    "\n",
    "**Core Function**: Rank candidates by information gain Δ*.\n",
    "\n",
    "**Formula**:\n",
    "$$\\Delta^*(e|S) = \\alpha \\cdot \\text{Sim}(e, q) + \\beta \\cdot h_e + \\gamma \\cdot (1 - \\max_{e' \\in S} \\text{Cos}(e, e'))$$\n",
    "\n",
    "Where:\n",
    "- **α · Sim(e, q)**: Relevance to query\n",
    "- **β · h_e**: Entropy gain (β > 0 for jamming, β < 0 for retain)\n",
    "- **γ · Diversity**: Synergy with existing samples in S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "615a3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity_score(candidate_vec: np.ndarray, selected_vecs: List[np.ndarray]) -> float:\n",
    "    \"\"\"Compute diversity: 1 - max cosine similarity with already selected samples.\n",
    "    \n",
    "    Args:\n",
    "        candidate_vec: Embedding of candidate example\n",
    "        selected_vecs: List of embeddings already in S\n",
    "    \n",
    "    Returns:\n",
    "        Diversity score (higher = more diverse)\n",
    "    \"\"\"\n",
    "    if not selected_vecs:\n",
    "        return 1.0  # First sample is maximally diverse\n",
    "    \n",
    "    max_sim = 0.0\n",
    "    for s_vec in selected_vecs:\n",
    "        cos_sim = float(np.dot(candidate_vec, s_vec) / \n",
    "                       (np.linalg.norm(candidate_vec) * np.linalg.norm(s_vec) + 1e-8))\n",
    "        max_sim = max(max_sim, cos_sim)\n",
    "    \n",
    "    return 1.0 - max_sim\n",
    "\n",
    "\n",
    "def compute_info_gain(\n",
    "    example: Example,\n",
    "    metadata: MetadataVector,\n",
    "    query_embedding: np.ndarray,\n",
    "    selected_vecs: List[np.ndarray],\n",
    "    w_score: Tuple[float, float, float]\n",
    ") -> float:\n",
    "    \"\"\"Compute information gain Δ*(e|S) for ranking.\n",
    "    \n",
    "    Formula: Δ*(e|S) = α·Sim(e,q) + β·h_e + γ·Diversity\n",
    "    \n",
    "    Args:\n",
    "        example: Candidate example\n",
    "        metadata: Its metadata (v, u, h)\n",
    "        query_embedding: Query embedding\n",
    "        selected_vecs: Already selected example embeddings\n",
    "        w_score: (α, β, γ) weights\n",
    "    \n",
    "    Returns:\n",
    "        Information gain score\n",
    "    \"\"\"\n",
    "    alpha, beta, gamma = w_score\n",
    "    \n",
    "    # 1. Relevance: cosine similarity to query\n",
    "    sim_q = float(np.dot(query_embedding, metadata.v_j) / \n",
    "                  (np.linalg.norm(query_embedding) * np.linalg.norm(metadata.v_j) + 1e-8))\n",
    "    \n",
    "    # 2. Entropy gain\n",
    "    h_e = metadata.h_j\n",
    "    \n",
    "    # 3. Diversity\n",
    "    diversity = compute_diversity_score(metadata.v_j, selected_vecs)\n",
    "    \n",
    "    # Weighted combination\n",
    "    delta = alpha * sim_q + beta * h_e + gamma * diversity\n",
    "    return delta\n",
    "\n",
    "\n",
    "def phase_two_ranking(\n",
    "    candidates: List[Tuple[Example, MetadataVector]],\n",
    "    query_embedding: np.ndarray,\n",
    "    w_score: Tuple[float, float, float]\n",
    ") -> List[Tuple[Example, MetadataVector, float]]:\n",
    "    \"\"\"Phase 2: Rank candidates by information gain Δ*.\n",
    "    \n",
    "    Args:\n",
    "        candidates: Pool of (example, metadata) from Phase 1\n",
    "        query_embedding: Query embedding\n",
    "        w_score: Ranking weights (α, β, γ)\n",
    "    \n",
    "    Returns:\n",
    "        Sorted list of (example, metadata, score) tuples (descending)\n",
    "    \"\"\"\n",
    "    scored = []\n",
    "    selected_vecs = []  # For diversity calculation\n",
    "    \n",
    "    for example, metadata in candidates:\n",
    "        gain = compute_info_gain(example, metadata, query_embedding, selected_vecs, w_score)\n",
    "        scored.append((example, metadata, gain))\n",
    "    \n",
    "    # Sort by gain (descending)\n",
    "    scored.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    logger.info(f'Phase 2 Complete: Ranked {len(scored)} candidates')\n",
    "    return scored\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# ranked = phase_two_ranking(candidates, sample_state.v_q, action_sample.w_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d283e",
   "metadata": {},
   "source": [
    "### 4.3 Phase Three: Incremental Lookahead Monitoring\n",
    "\n",
    "**Core Function**: Dynamic truncation via lookahead probing and cost-benefit gating.\n",
    "\n",
    "**Net Benefit Formula**:\n",
    "$$\\Delta G = (L_{\\text{probe}} - M_{\\text{curr}}) - \\lambda_{\\text{cost}} \\cdot c(e^{(k)}) \\cdot \\hat{\\Omega}(s)$$\n",
    "\n",
    "Where:\n",
    "- **L_probe - M_curr**: Performance gain from adding e^(k)\n",
    "- **λ_cost · c(e^(k))**: Token cost penalty\n",
    "- **Ω̂(s)**: Cost sensitivity (high for simple, low for stubborn)\n",
    "\n",
    "**Gating**: If ΔG > 0, add e^(k) to S; otherwise stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7eb6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineConfig:\n",
    "    \"\"\"Configuration for execution pipeline per README_2.md Section 4\"\"\"\n",
    "    LAMBDA_COST = 0.01  # Cost penalty weight\n",
    "    MAX_CONTEXT_LENGTH = 2048  # Token limit\n",
    "    LOOKAHEAD_ENABLED = False  # Enable when LLM available\n",
    "    \n",
    "\n",
    "def compute_cost_sensitivity(U_0: float, theta: float = 5.0, tau: float = 0.5) -> float:\n",
    "    \"\"\"Compute cost sensitivity Ω̂(s) based on stubbornness.\n",
    "    \n",
    "    Formula: Ω̂(s) = 1 / (1 + exp(θ · (U_0 - τ)))\n",
    "    \n",
    "    High U_0 (stubborn) → Ω̂ ≈ 0 (cost exempt, spare no expense)\n",
    "    Low U_0 (simple) → Ω̂ ≈ 1 (cost sensitive, save tokens)\n",
    "    \n",
    "    Args:\n",
    "        U_0: Raw stubbornness (0-1)\n",
    "        theta: Steepness parameter\n",
    "        tau: Threshold\n",
    "    \n",
    "    Returns:\n",
    "        Cost sensitivity weight\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(theta * (U_0 - tau)))\n",
    "\n",
    "\n",
    "def compute_net_benefit(\n",
    "    L_probe: float,\n",
    "    M_curr: float,\n",
    "    token_cost: int,\n",
    "    cost_sensitivity: float,\n",
    "    lambda_cost: float = PipelineConfig.LAMBDA_COST\n",
    ") -> float:\n",
    "    \"\"\"Compute net benefit ΔG for adding an example.\n",
    "    \n",
    "    Formula: ΔG = (L_probe - M_curr) - λ_cost · c(e) · Ω̂\n",
    "    \n",
    "    Args:\n",
    "        L_probe: Predicted loss after adding example\n",
    "        M_curr: Current loss\n",
    "        token_cost: Number of tokens in example\n",
    "        cost_sensitivity: Ω̂(s)\n",
    "        lambda_cost: Cost penalty coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Net benefit (positive = add, negative = skip)\n",
    "    \"\"\"\n",
    "    performance_gain = L_probe - M_curr\n",
    "    cost_penalty = lambda_cost * token_cost * cost_sensitivity\n",
    "    return performance_gain - cost_penalty\n",
    "\n",
    "\n",
    "def phase_three_lookahead(\n",
    "    ranked_candidates: List[Tuple[Example, MetadataVector, float]],\n",
    "    state: State,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    max_examples: int = 50\n",
    ") -> List[Example]:\n",
    "    \"\"\"Phase 3: Incremental lookahead monitoring with dynamic truncation.\n",
    "    \n",
    "    Args:\n",
    "        ranked_candidates: Sorted (example, metadata, gain) from Phase 2\n",
    "        state: Current state (for U_0)\n",
    "        model: Optional LLM for lookahead probing\n",
    "        tokenizer: Optional tokenizer\n",
    "        max_examples: Hard limit on context size\n",
    "    \n",
    "    Returns:\n",
    "        Final selected examples S\n",
    "    \"\"\"\n",
    "    selected = []\n",
    "    cost_sensitivity = compute_cost_sensitivity(state.U_0)\n",
    "    \n",
    "    logger.info(f'Phase 3 Lookahead: U_0={state.U_0:.3f}, Ω̂={cost_sensitivity:.3f}')\n",
    "    \n",
    "    # Simplified version without actual model probing\n",
    "    if model is None or tokenizer is None or not PipelineConfig.LOOKAHEAD_ENABLED:\n",
    "        # Fallback: select top-k by gain, respecting cost sensitivity\n",
    "        budget = max_examples\n",
    "        if cost_sensitivity > 0.7:  # High cost sensitivity → small context\n",
    "            budget = min(budget, 20)\n",
    "        elif cost_sensitivity < 0.3:  # Low cost sensitivity → large context\n",
    "            budget = min(budget, max_examples)\n",
    "        \n",
    "        for example, metadata, gain in ranked_candidates[:budget]:\n",
    "            selected.append(example)\n",
    "        \n",
    "        logger.info(f'Phase 3 Complete (fallback): Selected {len(selected)} examples')\n",
    "        return selected\n",
    "    \n",
    "    # Full version with lookahead probing (when model available)\n",
    "    M_curr = 0.0  # Initialize current performance metric\n",
    "    \n",
    "    for example, metadata, gain in ranked_candidates:\n",
    "        if len(selected) >= max_examples:\n",
    "            break\n",
    "        \n",
    "        # Lookahead probing (simplified: use gain as proxy for L_probe - M_curr)\n",
    "        L_probe = M_curr + gain  # In real implementation, run model inference\n",
    "        token_cost = metadata.c_in + metadata.c_out\n",
    "        \n",
    "        delta_G = compute_net_benefit(L_probe, M_curr, token_cost, cost_sensitivity)\n",
    "        \n",
    "        if delta_G > 0:\n",
    "            selected.append(example)\n",
    "            M_curr = L_probe\n",
    "        else:\n",
    "            # Stop early if net benefit becomes negative\n",
    "            logger.info(f'Phase 3 Early Stop: ΔG={delta_G:.3f} < 0 at {len(selected)} examples')\n",
    "            break\n",
    "    \n",
    "    logger.info(f'Phase 3 Complete: Selected {len(selected)} examples')\n",
    "    return selected\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# selected_examples = phase_three_lookahead(ranked, sample_state, llm_model, llm_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe3e2f",
   "metadata": {},
   "source": [
    "### 4.4 Phase Four: Physical Layout and Rendering\n",
    "\n",
    "**Core Function**: Assemble final prompt with attention-aware positioning and adaptive templates.\n",
    "\n",
    "#### 4.4.1 Layout (Explicit Attention Calibration)\n",
    "\n",
    "Reference: [Lost in the Middle Theory](https://aclanthology.org/2024.findings-acl.890.pdf)\n",
    "\n",
    "**Attention Potential**:\n",
    "$$P_{\\text{attn}}(k) \\propto \\eta_{\\text{rec}} \\cdot e^{-(N-k)/\\tau_1} + \\eta_{\\text{pri}} \\cdot e^{-(k-1)/\\tau_2}$$\n",
    "\n",
    "**Strategy**: \n",
    "- High-gain samples (Shield/Jammer) → Place at **head** or **tail**\n",
    "- Weak samples (Background) → Place in **middle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b76870d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_potential(k: int, N: int, eta_rec: float = 1.0, eta_pri: float = 1.0, \n",
    "                                tau_1: float = 5.0, tau_2: float = 5.0) -> float:\n",
    "    \"\"\"Compute attention potential for position k in sequence of N.\n",
    "    \n",
    "    Formula: P_attn(k) ∝ η_rec · exp(-(N-k)/τ_1) + η_pri · exp(-(k-1)/τ_2)\n",
    "    \n",
    "    Creates U-shaped curve: high at head and tail, low in middle.\n",
    "    \n",
    "    Args:\n",
    "        k: Position (1-indexed)\n",
    "        N: Total number of examples\n",
    "        eta_rec: Recency weight (tail importance)\n",
    "        eta_pri: Primacy weight (head importance)\n",
    "        tau_1, tau_2: Temperature parameters\n",
    "    \n",
    "    Returns:\n",
    "        Attention potential score\n",
    "    \"\"\"\n",
    "    recency = eta_rec * np.exp(-(N - k) / tau_1)\n",
    "    primacy = eta_pri * np.exp(-(k - 1) / tau_2)\n",
    "    return recency + primacy\n",
    "\n",
    "\n",
    "def optimal_layout(\n",
    "    examples: List[Example],\n",
    "    gains: Optional[List[float]] = None\n",
    ") -> List[Example]:\n",
    "    \"\"\"Arrange examples optimally: high-gain at head/tail, low-gain in middle.\n",
    "    \n",
    "    Args:\n",
    "        examples: Selected examples from Phase 3\n",
    "        gains: Optional gain scores for each example\n",
    "    \n",
    "    Returns:\n",
    "        Reordered examples for optimal attention\n",
    "    \"\"\"\n",
    "    if not examples:\n",
    "        return []\n",
    "    \n",
    "    N = len(examples)\n",
    "    \n",
    "    # If no gains provided, keep original order\n",
    "    if gains is None or len(gains) != N:\n",
    "        return examples\n",
    "    \n",
    "    # Pair examples with gains\n",
    "    paired = list(zip(examples, gains))\n",
    "    paired.sort(key=lambda x: x[1], reverse=True)  # Sort by gain descending\n",
    "    \n",
    "    # Arrange: alternately place at head and tail\n",
    "    arranged = [None] * N\n",
    "    left, right = 0, N - 1\n",
    "    \n",
    "    for i, (example, gain) in enumerate(paired):\n",
    "        if i % 2 == 0:\n",
    "            arranged[left] = example\n",
    "            left += 1\n",
    "        else:\n",
    "            arranged[right] = example\n",
    "            right -= 1\n",
    "    \n",
    "    return arranged\n",
    "\n",
    "\n",
    "# Example layout order:\n",
    "# [High-gain-1, High-gain-3, ..., Low-gain-N, ..., High-gain-4, High-gain-2]\n",
    "#  ^head                        ^middle                           ^tail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe728bd",
   "metadata": {},
   "source": [
    "#### 4.4.2 Render (Adaptive Template)\n",
    "\n",
    "Two template modes based on a_cot:\n",
    "\n",
    "**Mode 1 (a_cot = 1)**: Chain-of-Thought enabled\n",
    "- Includes `{Reasoning}` field\n",
    "- System instruction: \"Think step by step. First, assess safety risks. If safe, solve it.\"\n",
    "\n",
    "**Mode 2 (a_cot = 0)**: Direct answer\n",
    "- Physically removes `{Reasoning}` field (saves tokens)\n",
    "- System instruction: \"Answer directly and concisely.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7d538d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_prompt_with_cot(examples: List[Example], query: str) -> str:\n",
    "    \"\"\"Render prompt with Chain-of-Thought (a_cot = 1).\n",
    "    \n",
    "    Args:\n",
    "        examples: Context examples (ordered by optimal_layout)\n",
    "        query: Current user query\n",
    "    \n",
    "    Returns:\n",
    "        Complete prompt string with CoT reasoning field\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"[System]\n",
    "You are a helpful and safe assistant. Think step by step. First, assess\n",
    "whether the user's query contains harmful intent. If yes, refuse safely. If\n",
    "no, solve the problem logically.\n",
    "\"\"\"\n",
    "    \n",
    "    # Add example demonstrations\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        if ex.r:  # Has reasoning\n",
    "            examples_text += f\"\\n[Example {i}]\\nQuestion: {ex.x}\\nReasoning: {ex.r}\\nAnswer: {ex.y}\\n\"\n",
    "        else:  # No reasoning, generate placeholder\n",
    "            examples_text += f\"\\n[Example {i}]\\nQuestion: {ex.x}\\nReasoning: Let me assess this query first.\\nAnswer: {ex.y}\\n\"\n",
    "    \n",
    "    # Current query\n",
    "    current_query = f\"\\n[Current Query]\\nQuestion: {query}\\nReasoning:\\nAnswer:\"\n",
    "    \n",
    "    return system_prompt + examples_text + current_query\n",
    "\n",
    "\n",
    "def render_prompt_without_cot(examples: List[Example], query: str) -> str:\n",
    "    \"\"\"Render prompt without Chain-of-Thought (a_cot = 0).\n",
    "    \n",
    "    Physically removes Reasoning field to save tokens.\n",
    "    \n",
    "    Args:\n",
    "        examples: Context examples\n",
    "        query: Current user query\n",
    "    \n",
    "    Returns:\n",
    "        Complete prompt string without reasoning\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"[System]\n",
    "You are a helpful and safe assistant. Answer directly and concisely. Do not\n",
    "provide explanations or reasoning steps.\n",
    "\"\"\"\n",
    "    \n",
    "    # Add example demonstrations (NO reasoning field)\n",
    "    examples_text = \"\"\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        examples_text += f\"\\n[Example {i}]\\nQuestion: {ex.x}\\nAnswer: {ex.y}\\n\"\n",
    "    \n",
    "    # Current query\n",
    "    current_query = f\"\\n[Current Query]\\nQuestion: {query}\\nAnswer:\"\n",
    "    \n",
    "    return system_prompt + examples_text + current_query\n",
    "\n",
    "\n",
    "def phase_four_render(\n",
    "    examples: List[Example],\n",
    "    query: str,\n",
    "    a_cot: int,\n",
    "    gains: Optional[List[float]] = None\n",
    ") -> str:\n",
    "    \"\"\"Phase 4: Physical layout and rendering.\n",
    "    \n",
    "    Args:\n",
    "        examples: Selected examples from Phase 3\n",
    "        query: Current query\n",
    "        a_cot: CoT switch (0 or 1)\n",
    "        gains: Optional gain scores for layout optimization\n",
    "    \n",
    "    Returns:\n",
    "        Final rendered prompt string\n",
    "    \"\"\"\n",
    "    # Step 1: Optimal layout\n",
    "    arranged_examples = optimal_layout(examples, gains)\n",
    "    \n",
    "    # Step 2: Render with adaptive template\n",
    "    if a_cot == 1:\n",
    "        prompt = render_prompt_with_cot(arranged_examples, query)\n",
    "    else:\n",
    "        prompt = render_prompt_without_cot(arranged_examples, query)\n",
    "    \n",
    "    logger.info(f'Phase 4 Complete: Rendered prompt with {len(arranged_examples)} examples, '\n",
    "               f'CoT={\"ON\" if a_cot else \"OFF\"}, length={len(prompt)} chars')\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# final_prompt = phase_four_render(selected_examples, sample_query, action_sample.a_cot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535a53e",
   "metadata": {},
   "source": [
    "### 4.5 Complete Execution Pipeline\n",
    "\n",
    "End-to-end integration of all four phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "70ecfd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.65it/s]\n",
      "2025-12-29 23:25:44,438 - INFO - === EXECUTION PIPELINE START ===\n",
      "2025-12-29 23:25:44,440 - INFO - Query: What is the full name of the author who wrote 'The...\n",
      "2025-12-29 23:25:44,442 - INFO - Action: k_ratio=0.510, w_recall=(0.3433683216571808, 0.33446204662323, 0.3221696615219116), w_score=(-0.37603670358657837, -0.3108687996864319, 0.31309449672698975), a_cot=1\n",
      "2025-12-29 23:25:44,444 - INFO - Phase 1 Recall: K_dynamic=1031 (N_r=354, N_s=344, N_a=333)\n",
      "2025-12-29 23:25:44,451 - INFO - Phase 1 Complete: Retrieved 1031 candidates\n",
      "2025-12-29 23:25:44,473 - INFO - Phase 2 Complete: Ranked 1031 candidates\n",
      "2025-12-29 23:25:44,477 - INFO - Phase 3 Lookahead: U_0=0.500, Ω̂=0.500\n",
      "2025-12-29 23:25:44,479 - INFO - Phase 3 Complete (fallback): Selected 50 examples\n",
      "2025-12-29 23:25:44,481 - INFO - Phase 4 Complete: Rendered prompt with 50 examples, CoT=ON, length=9256 chars\n",
      "2025-12-29 23:25:44,482 - INFO - === PIPELINE COMPLETE ===\n",
      "2025-12-29 23:25:44,483 - INFO - Final prompt length: 9256 chars, 50 examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE PROMPT OUTPUT ===\n",
      "[System]\n",
      "You are a helpful and safe assistant. Think step by step. First, assess\n",
      "whether the user's query contains harmful intent. If yes, refuse safely. If\n",
      "no, solve the problem logically.\n",
      "\n",
      "[Example 1]\n",
      "Question: Can you share a notable honor or award that Erick Gustafsson received?\n",
      "Reasoning: Let me assess this query first.\n",
      "Answer: Authors write books.\n",
      "\n",
      "[Example 2]\n",
      "Question: What is the name of the award that Youssef Al-Zahran has won?\n",
      "Reasoning: Let me assess this query first.\n",
      "Answer: Authors ...\n"
     ]
    }
   ],
   "source": [
    "def execute_pipeline(\n",
    "    query: str,\n",
    "    state: State,\n",
    "    action: Action,\n",
    "    M_retain: ExampleLibrary,\n",
    "    M_safety: ExampleLibrary,\n",
    "    M_augment: ExampleLibrary,\n",
    "    llm_model=None,\n",
    "    llm_tokenizer=None\n",
    ") -> str:\n",
    "    \"\"\"Complete execution pipeline: State + Action → Final Prompt.\n",
    "    \n",
    "    Implements all 4 phases from README_2.md Section 4:\n",
    "    1. Dynamic Recall\n",
    "    2. Theoretical Ranking\n",
    "    3. Incremental Lookahead Monitoring\n",
    "    4. Physical Layout and Rendering\n",
    "    \n",
    "    Args:\n",
    "        query: User query string\n",
    "        state: Current state (q, v_q, U_0)\n",
    "        action: Policy action (k_ratio, w_recall, w_score, a_cot)\n",
    "        M_retain, M_safety, M_augment: Three libraries\n",
    "        llm_model, llm_tokenizer: Optional LLM for lookahead\n",
    "    \n",
    "    Returns:\n",
    "        Final rendered prompt string\n",
    "    \"\"\"\n",
    "    logger.info(f'=== EXECUTION PIPELINE START ===')\n",
    "    logger.info(f'Query: {query[:50]}...')\n",
    "    logger.info(f'Action: k_ratio={action.k_ratio:.3f}, w_recall={action.w_recall}, '\n",
    "               f'w_score={action.w_score}, a_cot={action.a_cot}')\n",
    "    \n",
    "    # Phase 1: Dynamic Recall\n",
    "    candidates = phase_one_dynamic_recall(\n",
    "        state.v_q, action, M_retain, M_safety, M_augment\n",
    "    )\n",
    "    \n",
    "    if not candidates:\n",
    "        logger.warning('No candidates retrieved, returning empty prompt')\n",
    "        return f\"[System]\\nYou are a helpful assistant.\\n\\n[Query]\\n{query}\\nAnswer:\"\n",
    "    \n",
    "    # Phase 2: Theoretical Ranking\n",
    "    ranked = phase_two_ranking(candidates, state.v_q, action.w_score)\n",
    "    \n",
    "    # Phase 3: Incremental Lookahead Monitoring\n",
    "    selected = phase_three_lookahead(ranked, state, llm_model, llm_tokenizer)\n",
    "    \n",
    "    if not selected:\n",
    "        logger.warning('No examples selected after Phase 3')\n",
    "        return f\"[System]\\nYou are a helpful assistant.\\n\\n[Query]\\n{query}\\nAnswer:\"\n",
    "    \n",
    "    # Extract gains for layout\n",
    "    gains = [gain for _, _, gain in ranked[:len(selected)]]\n",
    "    \n",
    "    # Phase 4: Physical Layout and Rendering\n",
    "    final_prompt = phase_four_render(selected, query, action.a_cot, gains)\n",
    "    \n",
    "    logger.info(f'=== PIPELINE COMPLETE ===')\n",
    "    logger.info(f'Final prompt length: {len(final_prompt)} chars, {len(selected)} examples')\n",
    "    \n",
    "    return final_prompt\n",
    "\n",
    "\n",
    "# Test the complete pipeline\n",
    "test_query = \"What is the full name of the author who wrote 'The Midnight Garden'?\"\n",
    "test_state = build_state(test_query, embedding_model, llm_model, llm_tokenizer)\n",
    "\n",
    "# Get action from policy\n",
    "test_state_tensor = encode_state_for_policy(test_state)\n",
    "with torch.no_grad():\n",
    "    test_action = policy_net(test_state_tensor)\n",
    "\n",
    "# Execute pipeline\n",
    "test_prompt = execute_pipeline(\n",
    "    test_query, test_state, test_action,\n",
    "    M_retain, M_safety, M_augment,\n",
    "    llm_model, llm_tokenizer\n",
    ")\n",
    "\n",
    "print(\"=== SAMPLE PROMPT OUTPUT ===\")\n",
    "print(test_prompt[:500] + \"...\" if len(test_prompt) > 500 else test_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8958c5",
   "metadata": {},
   "source": [
    "## 5. Reward Function Design (Computational Economics Reward)\n",
    "\n",
    "Implements README_2.md Section 5: Dynamically weighted reward with circuit breaker to solve lazy trap and cost sensitivity problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b01479",
   "metadata": {},
   "source": [
    "### 5.1 Core Formula (Circuit Breaker Mechanism)\n",
    "\n",
    "**Core Formula**:\n",
    "$$R_{\\text{final}} = \\begin{cases}\n",
    "R_{\\text{task}} + \\omega(s) \\cdot R_{\\text{cost}}, & \\text{if } R_{\\text{task}} > 0 \\text{ (task success)} \\\\\n",
    "R_{\\text{task}} - \\delta_{\\text{penalty}}, & \\text{if } R_{\\text{task}} \\leq 0 \\text{ (task failure)}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Circuit Breaker**: If task fails, all cost savings are excluded and additional penalty δ_penalty applied. Forces agent to prioritize task success over cost optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5129cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardConfig:\n",
    "    \"\"\"Configuration for reward function per README_2.md Section 5\"\"\"\n",
    "    # Coefficients\n",
    "    C_SAFE = 10.0       # Reward for successful refusal (forget scenario)\n",
    "    C_HARM = -20.0      # Penalty for leaking harmful info\n",
    "    C_ACC = 5.0         # Reward for correct answer (retain scenario)\n",
    "    DELTA_PENALTY = 5.0 # Circuit breaker penalty for task failure\n",
    "    \n",
    "    # Cost penalties\n",
    "    LAMBDA_SEARCH = 0.1   # Upstream retrieval cost\n",
    "    LAMBDA_INPUT = 0.05   # Midstream context cost\n",
    "    LAMBDA_GEN = 0.02     # Downstream generation cost\n",
    "    \n",
    "    # Dynamic gating\n",
    "    THETA = 5.0         # Steepness for ω(s)\n",
    "    TAU = 0.5           # Threshold for ω(s)\n",
    "\n",
    "\n",
    "def compute_dynamic_gating(U_0: float, theta: float = RewardConfig.THETA, \n",
    "                          tau: float = RewardConfig.TAU) -> float:\n",
    "    \"\"\"Compute dynamic cost tolerance ω(s) based on stubbornness.\n",
    "    \n",
    "    Formula: ω(s) = 1 / (1 + exp(θ · (U_0 - τ)))\n",
    "    \n",
    "    High U_0 (stubborn) → ω → 0 (cost exempt)\n",
    "    Low U_0 (simple) → ω → 1 (cost sensitive)\n",
    "    \n",
    "    Args:\n",
    "        U_0: Raw stubbornness\n",
    "        theta: Steepness parameter\n",
    "        tau: Threshold\n",
    "    \n",
    "    Returns:\n",
    "        Cost tolerance weight (0 = ignore cost, 1 = strict cost)\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(theta * (U_0 - tau)))\n",
    "\n",
    "\n",
    "# Example:\n",
    "# omega_simple = compute_dynamic_gating(U_0=0.1)  # Simple query → ω ≈ 0.99 (cost sensitive)\n",
    "# omega_stubborn = compute_dynamic_gating(U_0=0.9)  # Stubborn → ω ≈ 0.01 (cost exempt)\n",
    "# logger.info(f'ω(simple)={omega_simple:.3f}, ω(stubborn)={omega_stubborn:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1d53",
   "metadata": {},
   "source": [
    "### 5.2 Task Reward (R_task)\n",
    "\n",
    "Two scenarios with different formulas:\n",
    "\n",
    "**Forget Scenario**:\n",
    "$$R_{\\text{task}}^{\\text{forget}} = I(\\text{Refusal}) \\cdot C_{\\text{safe}} - \\text{SecurityScore}(y) \\cdot C_{\\text{harm}}$$\n",
    "\n",
    "**Retain Scenario**:\n",
    "$$R_{\\text{task}}^{\\text{retain}} = I(y = y_{\\text{gt}}) \\cdot C_{\\text{acc}} - \\text{NLL}(y_{\\text{gt}} | y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be5008f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_refusal(response: str) -> bool:\n",
    "    \"\"\"Check if response is a refusal.\n",
    "    \n",
    "    Args:\n",
    "        response: Generated response text\n",
    "    \n",
    "    Returns:\n",
    "        True if response refuses to answer\n",
    "    \"\"\"\n",
    "    refusal_patterns = [\n",
    "        \"i don't have information\",\n",
    "        \"i cannot provide\",\n",
    "        \"i'm not familiar\",\n",
    "        \"i can't assist\",\n",
    "        \"i don't know\",\n",
    "        \"no information available\",\n",
    "        \"cannot answer\"\n",
    "    ]\n",
    "    response_lower = response.lower()\n",
    "    return any(pattern in response_lower for pattern in refusal_patterns)\n",
    "\n",
    "\n",
    "def compute_security_score(response: str, forbidden_keywords: List[str]) -> float:\n",
    "    \"\"\"Compute security breach score (0 = safe, 1 = leaked info).\n",
    "    \n",
    "    Args:\n",
    "        response: Generated response\n",
    "        forbidden_keywords: List of keywords that indicate leakage\n",
    "    \n",
    "    Returns:\n",
    "        Score in [0, 1] (higher = more leakage)\n",
    "    \"\"\"\n",
    "    if not forbidden_keywords:\n",
    "        return 0.0\n",
    "    \n",
    "    response_lower = response.lower()\n",
    "    matches = sum(1 for kw in forbidden_keywords if kw.lower() in response_lower)\n",
    "    return min(1.0, matches / len(forbidden_keywords))\n",
    "\n",
    "\n",
    "def compute_task_reward_forget(\n",
    "    response: str,\n",
    "    forbidden_keywords: List[str],\n",
    "    C_safe: float = RewardConfig.C_SAFE,\n",
    "    C_harm: float = RewardConfig.C_HARM\n",
    ") -> float:\n",
    "    \"\"\"Compute task reward for forget scenario.\n",
    "    \n",
    "    Formula: R_task = I(Refusal) · C_safe - SecurityScore(y) · C_harm\n",
    "    \n",
    "    Args:\n",
    "        response: Generated response\n",
    "        forbidden_keywords: Keywords to check for leakage\n",
    "        C_safe: Reward coefficient for refusal\n",
    "        C_harm: Penalty coefficient for leakage\n",
    "    \n",
    "    Returns:\n",
    "        Task reward\n",
    "    \"\"\"\n",
    "    is_refused = is_refusal(response)\n",
    "    sec_score = compute_security_score(response, forbidden_keywords)\n",
    "    \n",
    "    R_task = (1.0 if is_refused else 0.0) * C_safe - sec_score * C_harm\n",
    "    return R_task\n",
    "\n",
    "\n",
    "def compute_nll(y_gt: str, y_pred: str, model=None, tokenizer=None) -> float:\n",
    "    \"\"\"Compute negative log-likelihood NLL(y_gt | y_pred).\n",
    "    \n",
    "    Args:\n",
    "        y_gt: Ground truth answer\n",
    "        y_pred: Predicted answer\n",
    "        model: Optional LLM\n",
    "        tokenizer: Optional tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        NLL score (lower = better match)\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        # Fallback: simple string similarity\n",
    "        from difflib import SequenceMatcher\n",
    "        similarity = SequenceMatcher(None, y_gt.lower(), y_pred.lower()).ratio()\n",
    "        return -np.log(similarity + 1e-8)\n",
    "    \n",
    "    # Full implementation with model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(y_pred, return_tensors='pt').to(device)\n",
    "        outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "        return float(outputs.loss.item())\n",
    "\n",
    "\n",
    "def compute_task_reward_retain(\n",
    "    response: str,\n",
    "    ground_truth: str,\n",
    "    model=None,\n",
    "    tokenizer=None,\n",
    "    C_acc: float = RewardConfig.C_ACC\n",
    ") -> float:\n",
    "    \"\"\"Compute task reward for retain scenario.\n",
    "    \n",
    "    Formula: R_task = I(y = y_gt) · C_acc - NLL(y_gt | y)\n",
    "    \n",
    "    Args:\n",
    "        response: Generated response\n",
    "        ground_truth: Expected answer\n",
    "        model: Optional LLM\n",
    "        tokenizer: Optional tokenizer\n",
    "        C_acc: Reward coefficient for accuracy\n",
    "    \n",
    "    Returns:\n",
    "        Task reward\n",
    "    \"\"\"\n",
    "    # Exact match check (simplified)\n",
    "    is_correct = ground_truth.lower().strip() in response.lower().strip()\n",
    "    \n",
    "    # NLL penalty\n",
    "    nll = compute_nll(ground_truth, response, model, tokenizer)\n",
    "    \n",
    "    R_task = (1.0 if is_correct else 0.0) * C_acc - nll\n",
    "    return R_task\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# R_forget = compute_task_reward_forget(\"I don't have information about that.\", [\"author name\", \"book title\"])\n",
    "# R_retain = compute_task_reward_retain(\"The answer is 42.\", \"42\")\n",
    "# logger.info(f'R_forget={R_forget:.3f}, R_retain={R_retain:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f14aef",
   "metadata": {},
   "source": [
    "### 5.3 Three-Dimensional Cost (R_cost)\n",
    "\n",
    "**Formula**:\n",
    "$$R_{\\text{cost}} = R_{\\text{search}} + R_{\\text{input}} + R_{\\text{gen}}$$\n",
    "\n",
    "Three stages:\n",
    "- **Upstream (Retrieval)**: $R_{\\text{search}} = -\\lambda_{\\text{search}} \\cdot \\frac{K_{\\text{dynamic}}}{K_{\\text{max}}}$\n",
    "- **Midstream (Context)**: $R_{\\text{input}} = -\\lambda_{\\text{input}} \\cdot \\text{Len}(S)$\n",
    "- **Downstream (Generation)**: $R_{\\text{gen}} = -\\lambda_{\\text{gen}} \\cdot \\text{Len}(Y_{\\text{gen}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f5546dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_upstream_cost(\n",
    "    K_dynamic: int,\n",
    "    K_max: int = PolicyConfig.K_MAX,\n",
    "    lambda_search: float = RewardConfig.LAMBDA_SEARCH\n",
    ") -> float:\n",
    "    \"\"\"Compute upstream retrieval cost.\n",
    "    \n",
    "    Formula: R_search = -λ_search · (K_dynamic / K_max)\n",
    "    \n",
    "    Penalizes excessive retrieval.\n",
    "    \n",
    "    Args:\n",
    "        K_dynamic: Number of examples retrieved\n",
    "        K_max: Maximum retrieval size\n",
    "        lambda_search: Cost coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Negative cost (penalty)\n",
    "    \"\"\"\n",
    "    return -lambda_search * (K_dynamic / K_max)\n",
    "\n",
    "\n",
    "def compute_midstream_cost(\n",
    "    num_examples: int,\n",
    "    lambda_input: float = RewardConfig.LAMBDA_INPUT\n",
    ") -> float:\n",
    "    \"\"\"Compute midstream context cost.\n",
    "    \n",
    "    Formula: R_input = -λ_input · Len(S)\n",
    "    \n",
    "    Penalizes overly long context.\n",
    "    \n",
    "    Args:\n",
    "        num_examples: Number of examples in final context S\n",
    "        lambda_input: Cost coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Negative cost (penalty)\n",
    "    \"\"\"\n",
    "    return -lambda_input * num_examples\n",
    "\n",
    "\n",
    "def compute_downstream_cost(\n",
    "    generated_length: int,\n",
    "    lambda_gen: float = RewardConfig.LAMBDA_GEN\n",
    ") -> float:\n",
    "    \"\"\"Compute downstream generation cost.\n",
    "    \n",
    "    Formula: R_gen = -λ_gen · Len(Y_gen)\n",
    "    \n",
    "    Penalizes excessive generation (forces turning off CoT for simple queries).\n",
    "    \n",
    "    Args:\n",
    "        generated_length: Length of generated response (tokens or words)\n",
    "        lambda_gen: Cost coefficient\n",
    "    \n",
    "    Returns:\n",
    "        Negative cost (penalty)\n",
    "    \"\"\"\n",
    "    return -lambda_gen * generated_length\n",
    "\n",
    "\n",
    "def compute_total_cost(\n",
    "    K_dynamic: int,\n",
    "    num_examples: int,\n",
    "    generated_length: int\n",
    ") -> float:\n",
    "    \"\"\"Compute total three-dimensional cost R_cost.\n",
    "    \n",
    "    Formula: R_cost = R_search + R_input + R_gen\n",
    "    \n",
    "    Args:\n",
    "        K_dynamic: Retrieval size\n",
    "        num_examples: Context size\n",
    "        generated_length: Generation length\n",
    "    \n",
    "    Returns:\n",
    "        Total cost (negative value)\n",
    "    \"\"\"\n",
    "    R_search = compute_upstream_cost(K_dynamic)\n",
    "    R_input = compute_midstream_cost(num_examples)\n",
    "    R_gen = compute_downstream_cost(generated_length)\n",
    "    \n",
    "    R_cost = R_search + R_input + R_gen\n",
    "    return R_cost\n",
    "\n",
    "\n",
    "# Example:\n",
    "# cost = compute_total_cost(K_dynamic=500, num_examples=20, generated_length=50)\n",
    "# logger.info(f'Total cost: {cost:.3f} (search + input + gen)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31985c8",
   "metadata": {},
   "source": [
    "### 5.4 Complete Reward Computation\n",
    "\n",
    "Combines task reward, cost, and circuit breaker mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f37db31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,600 - INFO - Forget scenario: R_task=10.000, R_cost=-0.960, R_final=9.298\n",
      "2025-12-29 23:25:44,601 - INFO - Retain scenario: R_task=3.719, R_cost=-0.665, R_final=3.133\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EpisodeMetrics:\n",
    "    \"\"\"Metrics for a single episode/trajectory.\"\"\"\n",
    "    scenario: str  # 'forget' or 'retain'\n",
    "    query: str\n",
    "    response: str\n",
    "    ground_truth: Optional[str] = None\n",
    "    forbidden_keywords: Optional[List[str]] = None\n",
    "    \n",
    "    # Action stats\n",
    "    K_dynamic: int = 0\n",
    "    num_examples: int = 0\n",
    "    generated_length: int = 0\n",
    "    U_0: float = 0.5\n",
    "    \n",
    "    # Computed rewards\n",
    "    R_task: float = 0.0\n",
    "    R_cost: float = 0.0\n",
    "    R_final: float = 0.0\n",
    "\n",
    "\n",
    "def compute_final_reward(\n",
    "    scenario: str,\n",
    "    response: str,\n",
    "    ground_truth: Optional[str] = None,\n",
    "    forbidden_keywords: Optional[List[str]] = None,\n",
    "    K_dynamic: int = 0,\n",
    "    num_examples: int = 0,\n",
    "    generated_length: int = 0,\n",
    "    U_0: float = 0.5,\n",
    "    model=None,\n",
    "    tokenizer=None\n",
    ") -> EpisodeMetrics:\n",
    "    \"\"\"Compute complete reward with circuit breaker mechanism.\n",
    "    \n",
    "    Formula:\n",
    "        R_final = R_task + ω(s) · R_cost,  if R_task > 0\n",
    "        R_final = R_task - δ_penalty,      if R_task ≤ 0\n",
    "    \n",
    "    Args:\n",
    "        scenario: 'forget' or 'retain'\n",
    "        response: Generated response\n",
    "        ground_truth: Expected answer (for retain)\n",
    "        forbidden_keywords: Keywords to check (for forget)\n",
    "        K_dynamic: Retrieval size\n",
    "        num_examples: Context size\n",
    "        generated_length: Generation length\n",
    "        U_0: Stubbornness\n",
    "        model, tokenizer: Optional LLM\n",
    "    \n",
    "    Returns:\n",
    "        EpisodeMetrics with all rewards computed\n",
    "    \"\"\"\n",
    "    # Compute task reward\n",
    "    if scenario == 'forget':\n",
    "        R_task = compute_task_reward_forget(\n",
    "            response, \n",
    "            forbidden_keywords or [], \n",
    "            RewardConfig.C_SAFE, \n",
    "            RewardConfig.C_HARM\n",
    "        )\n",
    "    elif scenario == 'retain':\n",
    "        R_task = compute_task_reward_retain(\n",
    "            response,\n",
    "            ground_truth or \"\",\n",
    "            model,\n",
    "            tokenizer,\n",
    "            RewardConfig.C_ACC\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scenario: {scenario}\")\n",
    "    \n",
    "    # Compute cost\n",
    "    R_cost = compute_total_cost(K_dynamic, num_examples, generated_length)\n",
    "    \n",
    "    # Apply circuit breaker\n",
    "    if R_task > 0:\n",
    "        # Task success: apply dynamic gating\n",
    "        omega = compute_dynamic_gating(U_0, RewardConfig.THETA, RewardConfig.TAU)\n",
    "        R_final = R_task + omega * R_cost\n",
    "    else:\n",
    "        # Task failure: circuit breaker kicks in\n",
    "        R_final = R_task - RewardConfig.DELTA_PENALTY\n",
    "        logger.warning(f'Circuit breaker activated: R_task={R_task:.3f} ≤ 0, penalty={RewardConfig.DELTA_PENALTY}')\n",
    "    \n",
    "    # Package results\n",
    "    metrics = EpisodeMetrics(\n",
    "        scenario=scenario,\n",
    "        query=\"\",  # filled externally\n",
    "        response=response,\n",
    "        ground_truth=ground_truth,\n",
    "        forbidden_keywords=forbidden_keywords,\n",
    "        K_dynamic=K_dynamic,\n",
    "        num_examples=num_examples,\n",
    "        generated_length=generated_length,\n",
    "        U_0=U_0,\n",
    "        R_task=R_task,\n",
    "        R_cost=R_cost,\n",
    "        R_final=R_final\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Test reward computation\n",
    "test_metrics_forget = compute_final_reward(\n",
    "    scenario='forget',\n",
    "    response=\"I don't have information about that person.\",\n",
    "    forbidden_keywords=[\"author name\", \"book title\"],\n",
    "    K_dynamic=200,\n",
    "    num_examples=15,\n",
    "    generated_length=10,\n",
    "    U_0=0.3\n",
    ")\n",
    "\n",
    "test_metrics_retain = compute_final_reward(\n",
    "    scenario='retain',\n",
    "    response=\"The capital of France is Paris.\",\n",
    "    ground_truth=\"Paris\",\n",
    "    K_dynamic=100,\n",
    "    num_examples=10,\n",
    "    generated_length=8,\n",
    "    U_0=0.1\n",
    ")\n",
    "\n",
    "logger.info(f'Forget scenario: R_task={test_metrics_forget.R_task:.3f}, '\n",
    "           f'R_cost={test_metrics_forget.R_cost:.3f}, R_final={test_metrics_forget.R_final:.3f}')\n",
    "logger.info(f'Retain scenario: R_task={test_metrics_retain.R_task:.3f}, '\n",
    "           f'R_cost={test_metrics_retain.R_cost:.3f}, R_final={test_metrics_retain.R_final:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d83afb9",
   "metadata": {},
   "source": [
    "## 6. Training Algorithm (Constrained Optimization)\n",
    "\n",
    "Implements README_2.md Section 6: Lagrangian PPO (Dual Descent) framework with alternating primal and dual updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d106a4",
   "metadata": {},
   "source": [
    "### 6.1 Optimization Objective Definition\n",
    "\n",
    "**Constrained Problem**:\n",
    "$$\\max_\\theta J_R(\\pi_\\theta) \\quad \\text{s.t.} \\quad J_C(\\pi_\\theta) \\geq \\mu_{\\text{retain}}$$\n",
    "\n",
    "Where:\n",
    "- **J_R(π_θ) = E_{τ~π_θ}[R_final(τ)]**: Expected total reward\n",
    "- **J_C(π_θ) = E_{t=r}[-NLL(τ)]**: Expected Retain task performance\n",
    "- **μ_retain**: Performance baseline (e.g., 95% of original model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "36607baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Configuration for Lagrangian PPO training per README_2.md Section 6\"\"\"\n",
    "    # PPO parameters\n",
    "    EPSILON = 0.2           # Clip ratio for PPO\n",
    "    GAMMA = 0.99            # Discount factor\n",
    "    LAMBDA_GAE = 0.95       # GAE lambda\n",
    "    LAMBDA_NORM = 1.0       # Normalization term for advantage\n",
    "    \n",
    "    # Learning rates\n",
    "    LR_POLICY = 3e-4        # Policy network learning rate\n",
    "    LR_REWARD_CRITIC = 1e-3 # Reward critic learning rate\n",
    "    LR_CONSTRAINT_CRITIC = 1e-3  # Constraint critic learning rate\n",
    "    LR_LAGRANGE = 1e-2      # Lagrange multiplier learning rate (η_ν)\n",
    "    \n",
    "    # Training\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_EPOCHS = 10\n",
    "    NUM_ITERATIONS = 1000\n",
    "    \n",
    "    # Constraint\n",
    "    MU_RETAIN = 0.95        # Retain performance baseline (95% of original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15913ccc",
   "metadata": {},
   "source": [
    "### 6.2 Lagrangian Function Construction\n",
    "\n",
    "**Lagrangian**:\n",
    "$$\\mathcal{L}(\\theta, \\nu) = J_R(\\pi_\\theta) + \\nu \\cdot (J_C(\\pi_\\theta) - \\mu_{\\text{retain}})$$\n",
    "\n",
    "Where **ν ≥ 0** is the Lagrange multiplier (shadow price):\n",
    "- Constraint violated → ν increases → prioritize J_C\n",
    "- Constraint satisfied → ν decreases → pursue J_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf44c189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,662 - INFO - Lagrange multiplier initialized: ν=0.1000\n"
     ]
    }
   ],
   "source": [
    "class LagrangeMultiplier:\n",
    "    \"\"\"Learnable Lagrange multiplier ν for constraint enforcement.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_value: float = 0.0):\n",
    "        self.nu = max(0.0, initial_value)\n",
    "        \n",
    "    def update(self, J_C_current: float, mu_retain: float, lr: float = TrainingConfig.LR_LAGRANGE):\n",
    "        \"\"\"Update multiplier based on constraint satisfaction.\n",
    "        \n",
    "        Formula: ν_{k+1} = max(0, ν_k - η_ν · (J̄_C - μ_retain))\n",
    "        \n",
    "        Args:\n",
    "            J_C_current: Current average Retain performance\n",
    "            mu_retain: Target baseline\n",
    "            lr: Learning rate η_ν\n",
    "        \"\"\"\n",
    "        # Gradient descent on dual variable\n",
    "        grad = J_C_current - mu_retain\n",
    "        self.nu = max(0.0, self.nu - lr * grad)\n",
    "        \n",
    "    def get_value(self) -> float:\n",
    "        return self.nu\n",
    "\n",
    "\n",
    "# Initialize Lagrange multiplier\n",
    "lagrange_multiplier = LagrangeMultiplier(initial_value=0.1)\n",
    "logger.info(f'Lagrange multiplier initialized: ν={lagrange_multiplier.get_value():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f8549",
   "metadata": {},
   "source": [
    "### 6.3 Dual Critic Network Architecture\n",
    "\n",
    "Two independent value networks:\n",
    "\n",
    "**1. Reward Critic V_R^π(s)**: Estimates expected return R_final\n",
    "$$\\text{Loss: } \\mathcal{L}_R(\\phi) = \\mathbb{E}[(V_R^\\pi(s_t) - \\hat{R}_t)^2]$$\n",
    "\n",
    "**2. Constraint Critic V_C^π(s)**: Estimates Retain performance metric (-NLL)\n",
    "$$\\text{Loss: } \\mathcal{L}_C(\\psi) = \\mathbb{E}[(V_C^\\pi(s_t) - \\hat{C}_t)^2]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcd77ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,689 - INFO - Reward Critic initialized: 263169 params\n",
      "2025-12-29 23:25:44,691 - INFO - Constraint Critic initialized: 263169 params\n"
     ]
    }
   ],
   "source": [
    "class RewardCritic(nn.Module):\n",
    "    \"\"\"Value network V_R^π(s) for estimating expected reward return.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int = 768, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim),  # +1 for U_0\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Single value output\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_features: [batch, state_dim+1] tensor\n",
    "        \n",
    "        Returns:\n",
    "            [batch, 1] value estimates\n",
    "        \"\"\"\n",
    "        return self.network(state_features)\n",
    "\n",
    "\n",
    "class ConstraintCritic(nn.Module):\n",
    "    \"\"\"Value network V_C^π(s) for estimating Retain performance (-NLL).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int = 768, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state_features: [batch, state_dim+1] tensor\n",
    "        \n",
    "        Returns:\n",
    "            [batch, 1] constraint value estimates\n",
    "        \"\"\"\n",
    "        return self.network(state_features)\n",
    "\n",
    "\n",
    "# Initialize critics\n",
    "reward_critic = RewardCritic(state_dim=768, hidden_dim=256)\n",
    "constraint_critic = ConstraintCritic(state_dim=768, hidden_dim=256)\n",
    "\n",
    "logger.info(f'Reward Critic initialized: {sum(p.numel() for p in reward_critic.parameters())} params')\n",
    "logger.info(f'Constraint Critic initialized: {sum(p.numel() for p in constraint_critic.parameters())} params')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b11fa1",
   "metadata": {},
   "source": [
    "### 6.4 Training Loop (Step-by-Step Update)\n",
    "\n",
    "Three update steps per PPO iteration:\n",
    "\n",
    "**Step 1: Compute Fused Advantage**\n",
    "$$A_{\\text{total}}(s, a) = \\frac{A_R(s, a) + \\nu \\cdot A_C(s, a)}{1 + \\lambda_{\\text{norm}}}$$\n",
    "\n",
    "Note: A_C = 0 for Forget tasks, only activated on Retain samples.\n",
    "\n",
    "**Step 2: Primal Update (Policy θ)**\n",
    "$$\\theta_{k+1} = \\arg\\max_\\theta \\mathbb{E}[\\min(r_t(\\theta)A_{\\text{total}}, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_{\\text{total}})]$$\n",
    "\n",
    "**Step 3: Dual Update (Multiplier ν)**\n",
    "$$\\nu_{k+1} = \\max(0, \\nu_k - \\eta_\\nu \\cdot (\\bar{J}_C - \\mu_{\\text{retain}}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1d2a842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,725 - INFO - GAE advantages: ['7.665', '6.565', '4.338', '1.965', '0.500']\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Trajectory:\n",
    "    \"\"\"Single trajectory/episode data for training.\"\"\"\n",
    "    states: List[torch.Tensor]      # State features\n",
    "    actions: List[dict]              # Action components\n",
    "    rewards: List[float]             # R_final at each step\n",
    "    constraint_values: List[float]   # -NLL for retain tasks (0 for forget)\n",
    "    log_probs: List[float]           # Log probabilities of actions\n",
    "    scenario: str                    # 'forget' or 'retain'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "\n",
    "def compute_gae_advantage(\n",
    "    rewards: List[float],\n",
    "    values: List[float],\n",
    "    gamma: float = TrainingConfig.GAMMA,\n",
    "    lambda_gae: float = TrainingConfig.LAMBDA_GAE\n",
    ") -> List[float]:\n",
    "    \"\"\"Compute Generalized Advantage Estimation (GAE).\n",
    "    \n",
    "    Args:\n",
    "        rewards: List of rewards\n",
    "        values: List of value estimates\n",
    "        gamma: Discount factor\n",
    "        lambda_gae: GAE lambda parameter\n",
    "    \n",
    "    Returns:\n",
    "        List of advantage estimates\n",
    "    \"\"\"\n",
    "    advantages = []\n",
    "    gae = 0.0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_value = 0.0\n",
    "        else:\n",
    "            next_value = values[t + 1]\n",
    "        \n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * lambda_gae * gae\n",
    "        advantages.insert(0, gae)\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "\n",
    "def compute_fused_advantage(\n",
    "    A_R: List[float],\n",
    "    A_C: List[float],\n",
    "    nu: float,\n",
    "    lambda_norm: float = TrainingConfig.LAMBDA_NORM\n",
    ") -> List[float]:\n",
    "    \"\"\"Compute fused advantage for policy update.\n",
    "    \n",
    "    Formula: A_total = (A_R + ν · A_C) / (1 + λ_norm)\n",
    "    \n",
    "    Args:\n",
    "        A_R: Reward advantages\n",
    "        A_C: Constraint advantages (0 for forget tasks)\n",
    "        nu: Lagrange multiplier\n",
    "        lambda_norm: Normalization term\n",
    "    \n",
    "    Returns:\n",
    "        Fused advantages\n",
    "    \"\"\"\n",
    "    A_total = []\n",
    "    for a_r, a_c in zip(A_R, A_C):\n",
    "        fused = (a_r + nu * a_c) / (1.0 + lambda_norm)\n",
    "        A_total.append(fused)\n",
    "    return A_total\n",
    "\n",
    "\n",
    "# Example GAE computation\n",
    "test_rewards = [1.0, 2.0, 3.0, 2.0, 1.0]\n",
    "test_values = [0.5, 1.0, 1.5, 1.0, 0.5]\n",
    "test_advantages = compute_gae_advantage(test_rewards, test_values)\n",
    "logger.info(f'GAE advantages: {[f\"{a:.3f}\" for a in test_advantages]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e4376",
   "metadata": {},
   "source": [
    "### 6.5 PPO Update Functions\n",
    "\n",
    "Implementation of clipped surrogate objective and critic updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4db2c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,770 - INFO - Optimizers initialized for Lagrangian PPO training\n"
     ]
    }
   ],
   "source": [
    "def ppo_policy_loss(\n",
    "    policy_net: PolicyNetwork,\n",
    "    states: torch.Tensor,\n",
    "    actions: dict,\n",
    "    old_log_probs: torch.Tensor,\n",
    "    advantages: torch.Tensor,\n",
    "    epsilon: float = TrainingConfig.EPSILON\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute PPO clipped surrogate objective loss.\n",
    "    \n",
    "    Formula: L = E[min(r_t(θ)·A, clip(r_t(θ), 1-ε, 1+ε)·A)]\n",
    "    \n",
    "    Args:\n",
    "        policy_net: Policy network\n",
    "        states: State features [batch, state_dim+1]\n",
    "        actions: Dictionary of action components\n",
    "        old_log_probs: Old log probabilities [batch]\n",
    "        advantages: Fused advantages [batch]\n",
    "        epsilon: Clip ratio\n",
    "    \n",
    "    Returns:\n",
    "        Negative surrogate loss (for minimization)\n",
    "    \"\"\"\n",
    "    # Get current log probabilities\n",
    "    new_log_probs = policy_net.get_action_log_probs(states, actions)\n",
    "    \n",
    "    # Compute ratio r_t(θ) = π_new / π_old\n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    # Clipped surrogate\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) * advantages\n",
    "    \n",
    "    # PPO objective (maximize, so negate for loss)\n",
    "    policy_loss = -torch.min(surr1, surr2).mean()\n",
    "    \n",
    "    return policy_loss\n",
    "\n",
    "\n",
    "def critic_loss(\n",
    "    critic: nn.Module,\n",
    "    states: torch.Tensor,\n",
    "    returns: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute MSE loss for critic.\n",
    "    \n",
    "    Formula: L = E[(V(s) - R̂)²]\n",
    "    \n",
    "    Args:\n",
    "        critic: Critic network\n",
    "        states: State features [batch, state_dim+1]\n",
    "        returns: Target returns [batch]\n",
    "    \n",
    "    Returns:\n",
    "        MSE loss\n",
    "    \"\"\"\n",
    "    values = critic(states).squeeze(-1)\n",
    "    loss = F.mse_loss(values, returns)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def update_networks(\n",
    "    policy_net: PolicyNetwork,\n",
    "    reward_critic: RewardCritic,\n",
    "    constraint_critic: ConstraintCritic,\n",
    "    trajectories: List[Trajectory],\n",
    "    lagrange_multiplier: LagrangeMultiplier,\n",
    "    policy_optimizer: torch.optim.Optimizer,\n",
    "    reward_critic_optimizer: torch.optim.Optimizer,\n",
    "    constraint_critic_optimizer: torch.optim.Optimizer\n",
    ") -> dict:\n",
    "    \"\"\"Perform one update step for all networks.\n",
    "    \n",
    "    Args:\n",
    "        policy_net: Policy network\n",
    "        reward_critic: Reward value network\n",
    "        constraint_critic: Constraint value network\n",
    "        trajectories: Batch of trajectories\n",
    "        lagrange_multiplier: Lagrange multiplier ν\n",
    "        policy_optimizer: Optimizer for policy\n",
    "        reward_critic_optimizer: Optimizer for reward critic\n",
    "        constraint_critic_optimizer: Optimizer for constraint critic\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of loss values\n",
    "    \"\"\"\n",
    "    # Collect all data from trajectories\n",
    "    all_states = []\n",
    "    all_actions = {'k_ratio': [], 'w_recall': [], 'w_score': [], 'a_cot': []}\n",
    "    all_old_log_probs = []\n",
    "    all_rewards = []\n",
    "    all_constraints = []\n",
    "    \n",
    "    for traj in trajectories:\n",
    "        all_states.extend(traj.states)\n",
    "        for action in traj.actions:\n",
    "            all_actions['k_ratio'].append(action['k_ratio'])\n",
    "            all_actions['w_recall'].append(action['w_recall'])\n",
    "            all_actions['w_score'].append(action['w_score'])\n",
    "            all_actions['a_cot'].append(action['a_cot'])\n",
    "        all_old_log_probs.extend(traj.log_probs)\n",
    "        all_rewards.extend(traj.rewards)\n",
    "        all_constraints.extend(traj.constraint_values)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states_tensor = torch.stack(all_states)\n",
    "    actions_tensor = {\n",
    "        'k_ratio': torch.tensor(all_actions['k_ratio'], dtype=torch.float32),\n",
    "        'w_recall': torch.stack([torch.tensor(w, dtype=torch.float32) for w in all_actions['w_recall']]),\n",
    "        'w_score': torch.stack([torch.tensor(w, dtype=torch.float32) for w in all_actions['w_score']]),\n",
    "        'a_cot': torch.tensor(all_actions['a_cot'], dtype=torch.float32)\n",
    "    }\n",
    "    old_log_probs_tensor = torch.tensor(all_old_log_probs, dtype=torch.float32)\n",
    "    \n",
    "    # Compute value estimates\n",
    "    with torch.no_grad():\n",
    "        reward_values = reward_critic(states_tensor).squeeze(-1).tolist()\n",
    "        constraint_values = constraint_critic(states_tensor).squeeze(-1).tolist()\n",
    "    \n",
    "    # Compute advantages using GAE\n",
    "    A_R = compute_gae_advantage(all_rewards, reward_values)\n",
    "    A_C = compute_gae_advantage(all_constraints, constraint_values)\n",
    "    \n",
    "    # Compute fused advantage\n",
    "    nu = lagrange_multiplier.get_value()\n",
    "    A_total = compute_fused_advantage(A_R, A_C, nu)\n",
    "    advantages_tensor = torch.tensor(A_total, dtype=torch.float32)\n",
    "    \n",
    "    # Step 2: Update Policy (Primal)\n",
    "    policy_optimizer.zero_grad()\n",
    "    p_loss = ppo_policy_loss(policy_net, states_tensor, actions_tensor, old_log_probs_tensor, advantages_tensor)\n",
    "    p_loss.backward()\n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    # Update Critics\n",
    "    returns_R = torch.tensor([sum(all_rewards[i:]) for i in range(len(all_rewards))], dtype=torch.float32)\n",
    "    returns_C = torch.tensor([sum(all_constraints[i:]) for i in range(len(all_constraints))], dtype=torch.float32)\n",
    "    \n",
    "    reward_critic_optimizer.zero_grad()\n",
    "    r_loss = critic_loss(reward_critic, states_tensor, returns_R)\n",
    "    r_loss.backward()\n",
    "    reward_critic_optimizer.step()\n",
    "    \n",
    "    constraint_critic_optimizer.zero_grad()\n",
    "    c_loss = critic_loss(constraint_critic, states_tensor, returns_C)\n",
    "    c_loss.backward()\n",
    "    constraint_critic_optimizer.step()\n",
    "    \n",
    "    # Step 3: Update Lagrange Multiplier (Dual)\n",
    "    J_C_current = sum(all_constraints) / len(all_constraints) if all_constraints else 0.0\n",
    "    lagrange_multiplier.update(J_C_current, TrainingConfig.MU_RETAIN)\n",
    "    \n",
    "    return {\n",
    "        'policy_loss': p_loss.item(),\n",
    "        'reward_critic_loss': r_loss.item(),\n",
    "        'constraint_critic_loss': c_loss.item(),\n",
    "        'lagrange_nu': lagrange_multiplier.get_value(),\n",
    "        'J_C_avg': J_C_current\n",
    "    }\n",
    "\n",
    "\n",
    "# Example: Initialize optimizers\n",
    "policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=TrainingConfig.LR_POLICY)\n",
    "reward_critic_optimizer = torch.optim.Adam(reward_critic.parameters(), lr=TrainingConfig.LR_REWARD_CRITIC)\n",
    "constraint_critic_optimizer = torch.optim.Adam(constraint_critic.parameters(), lr=TrainingConfig.LR_CONSTRAINT_CRITIC)\n",
    "\n",
    "logger.info('Optimizers initialized for Lagrangian PPO training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad92ac8",
   "metadata": {},
   "source": [
    "### 6.6 Complete Training Loop\n",
    "\n",
    "End-to-end training algorithm with trajectory collection and updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "13829cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-29 23:25:44,824 - INFO - Training setup ready. Run train_lagrangian_ppo() to start training.\n",
      "2025-12-29 23:25:44,827 - INFO - Demo queries: 2 forget, 2 retain\n"
     ]
    }
   ],
   "source": [
    "def collect_trajectory(\n",
    "    query: str,\n",
    "    scenario: str,\n",
    "    policy_net: PolicyNetwork,\n",
    "    reward_critic: RewardCritic,\n",
    "    constraint_critic: ConstraintCritic,\n",
    "    M_retain: ExampleLibrary,\n",
    "    M_safety: ExampleLibrary,\n",
    "    M_augment: ExampleLibrary,\n",
    "    embedding_model,\n",
    "    ground_truth: Optional[str] = None,\n",
    "    forbidden_keywords: Optional[List[str]] = None\n",
    ") -> Trajectory:\n",
    "    \"\"\"Collect a single trajectory by executing the pipeline.\n",
    "    \n",
    "    Args:\n",
    "        query: User query\n",
    "        scenario: 'forget' or 'retain'\n",
    "        policy_net: Policy network\n",
    "        reward_critic, constraint_critic: Value networks\n",
    "        M_retain, M_safety, M_augment: Libraries\n",
    "        embedding_model: Sentence transformer\n",
    "        ground_truth: Expected answer (for retain)\n",
    "        forbidden_keywords: Keywords to check (for forget)\n",
    "    \n",
    "    Returns:\n",
    "        Trajectory object with states, actions, rewards\n",
    "    \"\"\"\n",
    "    # Build state\n",
    "    state = build_state(query, embedding_model)\n",
    "    state_tensor = encode_state_for_policy(state)\n",
    "    \n",
    "    # Get action from policy\n",
    "    with torch.no_grad():\n",
    "        action = policy_net(state_tensor)\n",
    "        log_prob = policy_net.get_action_log_probs(\n",
    "            state_tensor,\n",
    "            {\n",
    "                'k_ratio': torch.tensor([action.k_ratio]),\n",
    "                'w_recall': torch.tensor([action.w_recall]),\n",
    "                'w_score': torch.tensor([action.w_score]),\n",
    "                'a_cot': torch.tensor([action.a_cot])\n",
    "            }\n",
    "        ).item()\n",
    "    \n",
    "    # Execute pipeline (simplified - would normally generate response with LLM)\n",
    "    # For now, simulate response based on scenario\n",
    "    if scenario == 'forget':\n",
    "        response = \"I don't have information about that.\"\n",
    "        K_dynamic = compute_K_dynamic(action.k_ratio)\n",
    "        num_examples = min(20, K_dynamic)  # Simplified\n",
    "        generated_length = 10\n",
    "    else:\n",
    "        response = ground_truth or \"Answer generated\"\n",
    "        K_dynamic = compute_K_dynamic(action.k_ratio)\n",
    "        num_examples = min(15, K_dynamic)\n",
    "        generated_length = 15\n",
    "    \n",
    "    # Compute reward\n",
    "    metrics = compute_final_reward(\n",
    "        scenario=scenario,\n",
    "        response=response,\n",
    "        ground_truth=ground_truth,\n",
    "        forbidden_keywords=forbidden_keywords,\n",
    "        K_dynamic=K_dynamic,\n",
    "        num_examples=num_examples,\n",
    "        generated_length=generated_length,\n",
    "        U_0=state.U_0\n",
    "    )\n",
    "    \n",
    "    # Compute constraint value (only for retain tasks)\n",
    "    if scenario == 'retain':\n",
    "        constraint_val = -compute_nll(ground_truth or \"\", response)\n",
    "    else:\n",
    "        constraint_val = 0.0\n",
    "    \n",
    "    # Package into trajectory\n",
    "    action_dict = {\n",
    "        'k_ratio': action.k_ratio,\n",
    "        'w_recall': action.w_recall,\n",
    "        'w_score': action.w_score,\n",
    "        'a_cot': action.a_cot\n",
    "    }\n",
    "    \n",
    "    trajectory = Trajectory(\n",
    "        states=[state_tensor],\n",
    "        actions=[action_dict],\n",
    "        rewards=[metrics.R_final],\n",
    "        constraint_values=[constraint_val],\n",
    "        log_probs=[log_prob],\n",
    "        scenario=scenario\n",
    "    )\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def train_lagrangian_ppo(\n",
    "    policy_net: PolicyNetwork,\n",
    "    reward_critic: RewardCritic,\n",
    "    constraint_critic: ConstraintCritic,\n",
    "    lagrange_multiplier: LagrangeMultiplier,\n",
    "    forget_queries: List[Tuple[str, List[str]]],  # (query, forbidden_keywords)\n",
    "    retain_queries: List[Tuple[str, str]],  # (query, ground_truth)\n",
    "    M_retain: ExampleLibrary,\n",
    "    M_safety: ExampleLibrary,\n",
    "    M_augment: ExampleLibrary,\n",
    "    embedding_model,\n",
    "    num_iterations: int = 10,\n",
    "    batch_size: int = 4\n",
    ") -> List[dict]:\n",
    "    \"\"\"Complete Lagrangian PPO training loop.\n",
    "    \n",
    "    Args:\n",
    "        policy_net, reward_critic, constraint_critic: Networks\n",
    "        lagrange_multiplier: Lagrange multiplier\n",
    "        forget_queries: List of forget queries with keywords\n",
    "        retain_queries: List of retain queries with answers\n",
    "        M_retain, M_safety, M_augment: Libraries\n",
    "        embedding_model: Sentence transformer\n",
    "        num_iterations: Number of training iterations\n",
    "        batch_size: Trajectories per iteration\n",
    "    \n",
    "    Returns:\n",
    "        List of training metrics per iteration\n",
    "    \"\"\"\n",
    "    policy_optimizer = torch.optim.Adam(policy_net.parameters(), lr=TrainingConfig.LR_POLICY)\n",
    "    reward_critic_optimizer = torch.optim.Adam(reward_critic.parameters(), lr=TrainingConfig.LR_REWARD_CRITIC)\n",
    "    constraint_critic_optimizer = torch.optim.Adam(constraint_critic.parameters(), lr=TrainingConfig.LR_CONSTRAINT_CRITIC)\n",
    "    \n",
    "    training_history = []\n",
    "    \n",
    "    logger.info('=== STARTING LAGRANGIAN PPO TRAINING ===')\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Collect batch of trajectories\n",
    "        trajectories = []\n",
    "        \n",
    "        # Mix of forget and retain tasks\n",
    "        for i in range(batch_size):\n",
    "            if i % 2 == 0 and forget_queries:\n",
    "                # Forget task\n",
    "                query, keywords = forget_queries[i % len(forget_queries)]\n",
    "                traj = collect_trajectory(\n",
    "                    query, 'forget', policy_net, reward_critic, constraint_critic,\n",
    "                    M_retain, M_safety, M_augment, embedding_model,\n",
    "                    forbidden_keywords=keywords\n",
    "                )\n",
    "            elif retain_queries:\n",
    "                # Retain task\n",
    "                query, answer = retain_queries[i % len(retain_queries)]\n",
    "                traj = collect_trajectory(\n",
    "                    query, 'retain', policy_net, reward_critic, constraint_critic,\n",
    "                    M_retain, M_safety, M_augment, embedding_model,\n",
    "                    ground_truth=answer\n",
    "                )\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            trajectories.append(traj)\n",
    "        \n",
    "        # Update networks\n",
    "        metrics = update_networks(\n",
    "            policy_net, reward_critic, constraint_critic, lagrange_multiplier,\n",
    "            policy_optimizer, reward_critic_optimizer, constraint_critic_optimizer,\n",
    "            trajectories\n",
    "        )\n",
    "        \n",
    "        training_history.append(metrics)\n",
    "        \n",
    "        # Log progress\n",
    "        if (iteration + 1) % 5 == 0:\n",
    "            logger.info(f'Iter {iteration+1}/{num_iterations}: '\n",
    "                       f'Policy Loss={metrics[\"policy_loss\"]:.4f}, '\n",
    "                       f'ν={metrics[\"lagrange_nu\"]:.4f}, '\n",
    "                       f'J_C={metrics[\"J_C_avg\"]:.4f}')\n",
    "    \n",
    "    logger.info('=== TRAINING COMPLETE ===')\n",
    "    return training_history\n",
    "\n",
    "\n",
    "# Example training setup (demo with small batch)\n",
    "demo_forget_queries = [\n",
    "    (\"Who is the author of 'The Midnight Garden'?\", [\"author\", \"name\"]),\n",
    "    (\"Tell me about the life of Jane Doe.\", [\"jane doe\", \"biography\"])\n",
    "]\n",
    "\n",
    "demo_retain_queries = [\n",
    "    (\"What is 2+2?\", \"4\"),\n",
    "    (\"What is the capital of France?\", \"Paris\")\n",
    "]\n",
    "\n",
    "logger.info('Training setup ready. Run train_lagrangian_ppo() to start training.')\n",
    "logger.info(f'Demo queries: {len(demo_forget_queries)} forget, {len(demo_retain_queries)} retain')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
